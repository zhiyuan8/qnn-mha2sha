{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMET Quantization workflow for Llama 3.2 3B Context Length 4K\n",
    "\n",
    "This notebook shows a working code example of how to use AIMET to quantize LlamaV3.2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Required packages\n",
    "The notebook assumes AIMET and LLamaV3.2 related packages are already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages only if running in jupyter notebook mode\n",
    "# if hasattr(__builtins__,'__IPYTHON__'):\n",
    "#     !sudo -H apt-get -qq update\n",
    "#     !sudo -H apt-get -qq install libc++-dev\n",
    "#     !sudo -H pip install --quiet --upgrade --root-user-action=ignore --no-cache-dir transformers==4.43.2\n",
    "#     !sudo -H pip install --quiet --upgrade --root-user-action=ignore --no-cache-dir tokenizers==0.19.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall flow\n",
    "This notebook covers the following\n",
    "1. Setting QNN SDK\n",
    "2. Instantiate and adapt FP32 model\n",
    "3. Complete the last step(s) of model adaptation\n",
    "4. Convert FP32 model to FP16\n",
    "5. Model Evaluation\n",
    "6. Model Sample Input\n",
    "7. Prepare model using AIMET model preparer pro\n",
    "8. Quantization\n",
    "9. Export\n",
    "\n",
    "\n",
    "### What this notebook is not \n",
    "* This notebook is not intended to show the full scope of optimization. For example, the flow will not use QAT, KD-QAT as deliberate choice to have the notebook execute more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Setting QNN SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "QNN_SDK_ROOT='/opt/qcom/aistack/qairt/2.28.0.241029/' # QNN 2.28\n",
    "assert QNN_SDK_ROOT != '', 'Please point the QNN_SDK_ROOT variable to your QNN SDK'\n",
    "lib_clang_path = os.path.join(QNN_SDK_ROOT, 'lib', 'x86_64-linux-clang')\n",
    "sys.path.insert(0, QNN_SDK_ROOT + '/lib/python')\n",
    "LD_LIBRARY_PATH = os.getenv('LD_LIBRARY_PATH', None)\n",
    "os.environ['LD_LIBRARY_PATH'] = lib_clang_path + ':' + LD_LIBRARY_PATH if LD_LIBRARY_PATH is not None else lib_clang_path\n",
    "enable_fp16 = False # Flag to enable e2e fp16 flow, set to false to set fp32 flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Setting NSP Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../common/')\n",
    "from utilities.nsptargets import NspTargets\n",
    "\n",
    "# Windows GEN 2 is supported for this notebook\n",
    "nsp_target = NspTargets.Windows.GEN2\n",
    "\n",
    "## Select quantsim config based on target\n",
    "## HACK: We should consider to change this as some fixed config in the future\n",
    "htp_config_file = f'/venv/lib/python3.10/site-packages/aimet_common/quantsim_config/htp_quantsim_config_{nsp_target.dsp_arch}.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Instantiate and adapt FP32 model\n",
    "\n",
    "Now, we will use our own unique implementation for the Qwen3 model. Use our own implementation to instantiate the FP32 model.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "import torch\n",
    "from hf_tokenizers import Tokenizer\n",
    "sys.path.append(\"../../../qwen2_torch\")\n",
    "from modeling_qwen2 import QNNQwen2, QNNLLMUtils\n",
    "\n",
    "model_name = 'qwen2'\n",
    "model_id = 'Qwen/Qwen2.5-1.5B-Instruct'\n",
    "\n",
    "cache_dir = './cache_dir'\n",
    "output_dir = f'./output_dir_{os.path.basename(model_id)}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "qnn_model = QNNQwen2.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "tokenizer = Tokenizer(\"/workspace/qnn-expr/experiments/qwen2_tokenizer.json\")\n",
    "qnn_model.qnn_init()\n",
    "config = qnn_model.config\n",
    "SEQ_LEN = 2073\n",
    "KV_LEN = 4096 - SEQ_LEN\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "qnn_model.to(device)\n",
    "qnn_llm_utils = QNNLLMUtils(SEQ_LEN, KV_LEN, device, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Direct verification\n",
    "\n",
    "We use the models loaded, and directly check if the model can do correct inference. Ensure the model can create the reasonable outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = 151645\n",
    "eos_tokens = {151645, 151643}\n",
    "input_text = (\n",
    "    \"<|im_start|>user\\nIntroduce Newton's first law of motion. Be short and concise.<|im_end|>\"\n",
    "    \"\\n<|im_start|>assistant\\n\"\n",
    ")\n",
    "input_ids_list = tokenizer.encode(input_text)  # a list of ints\n",
    "\n",
    "n_past = 0\n",
    "curr_len = len(input_ids_list)\n",
    "\n",
    "if curr_len < SEQ_LEN:\n",
    "    input_ids_list = input_ids_list + [eos_token_id] * (SEQ_LEN - curr_len)\n",
    "\n",
    "input_ids = torch.tensor(input_ids_list, dtype=torch.long, device=device).unsqueeze(0)\n",
    "attention_mask = qnn_llm_utils.get_attention_mask(n_past, curr_len)\n",
    "position_ids = qnn_llm_utils.get_position_ids(n_past, SEQ_LEN)\n",
    "cos, sin = qnn_llm_utils.get_cos_sin(attention_mask, position_ids)  # attn_mask as dummpy input to get device\n",
    "all_layer_kv_caches = qnn_llm_utils.get_kv_cache()\n",
    "last_token_indices = torch.tensor([n_past + curr_len - 1], dtype=torch.long, device=device)\n",
    "\n",
    "generated_ids = []\n",
    "\n",
    "for i in range(10):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = qnn_model(input_ids, cos, sin, attention_mask, all_layer_kv_caches)\n",
    "        next_token_logits = outputs[0][:, last_token_indices, :]\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1).item()\n",
    "        generated_ids.append(next_token_id)\n",
    "\n",
    "        ## update the inputs for next token\n",
    "        all_layer_kv_caches = qnn_llm_utils.update_kv_cache(all_layer_kv_caches, outputs[1:], n_past, curr_len)\n",
    "        n_past += curr_len\n",
    "        curr_len = 1\n",
    "        next_input_ids = [next_token_id] + [eos_token_id] * (SEQ_LEN - 1)\n",
    "        input_ids = torch.tensor(next_input_ids, dtype=torch.long, device=device).unsqueeze(\n",
    "            0\n",
    "        )  # set bs_size = 1 manually\n",
    "        last_token_indices = torch.tensor([curr_len - 1], dtype=torch.long, device=device)\n",
    "        attention_mask = qnn_llm_utils.get_attention_mask(n_past, curr_len)\n",
    "        position_ids = qnn_llm_utils.get_position_ids(n_past, SEQ_LEN)\n",
    "        cos, sin = qnn_llm_utils.get_cos_sin(attention_mask, position_ids)\n",
    "\n",
    "    if next_token_id in eos_tokens:\n",
    "        break\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    "### 3. Convert FP32 model to FP16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " The following code contains a temporary measure needed to maintain model accuracy when converting to FP16. RMSnorm operators are very sensitive to changes in bitwidth, and must upcast the input tensor to FP32 first. Once the QNN converter is able to recognize and coalesce RMSnorm operations, this upconversion will be handled automatically. Until then, we must insert operators to upcast tensors to FP32 before the first RMSnorm component operation, and downcast tensors back to FP16 once all RMSnorm component ops are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch import elementwise_ops\n",
    "\n",
    "class PreCast(torch.nn.Module):\n",
    "    def __init__(self, module, dtype):\n",
    "        super(PreCast, self).__init__()\n",
    "        self.module = module\n",
    "        self.upcast = elementwise_ops.Cast(dtype)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        casted_inputs = [self.upcast(input) for input in inputs]\n",
    "        return self.module(*casted_inputs)\n",
    "\n",
    "class PostCast(torch.nn.Module):\n",
    "    def __init__(self, module, dtype):\n",
    "        super(PostCast, self).__init__()\n",
    "        self.module = module\n",
    "        self.downcast = elementwise_ops.Cast(dtype)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        output = self.module(*inputs)\n",
    "        casted_output = self.downcast(output)\n",
    "        return casted_output\n",
    "    \n",
    "# Helper function to convert FP32 model to FP16\n",
    "# Inserts upcast and downcast operators around RMSnorm operators if found in the graph\n",
    "## NEXA: After model preparation, rms norm will have two ops norm_Pow and norm_Mul_1,\n",
    "#        This is why we are using the norm_Pow, and norm_Mul_1.\n",
    "def convert_model_to_fp16(model):\n",
    "    model.half()\n",
    "    for name, module in model.named_modules():\n",
    "        if name.endswith(\"norm_Pow\"):\n",
    "            setattr(model, name, PreCast(module, torch.float32))\n",
    "        if name.endswith(\"norm_Mul_1\"):\n",
    "            setattr(model, name, PostCast(module, torch.float16))\n",
    "            \n",
    "# Helper function to convert FP16 model back to FP32\n",
    "# Removes upcast and downcast operators inserted by convert_model_to_fp16, if present\n",
    "def convert_model_to_fp32(model):\n",
    "    model.float()\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if name.endswith(\"norm_Pow\"):\n",
    "            setattr(model, name, module.module)\n",
    "        if name.endswith(\"norm_Mul_1\"):\n",
    "            setattr(model, name, module.module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = qnn_model\n",
    "if(enable_fp16):\n",
    "   convert_model_to_fp16(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from aimet_torch.pro.utils.profiler import event_marker\n",
    "\n",
    "test_dataset = load_dataset(\"nexa4ai/qwen3_wiki_calib\", split=\"test\")\n",
    "num_total_batches = len(test_dataset)\n",
    "\n",
    "\n",
    "def ppl_eval(test_dataset, model, num_batches=0, data_type=torch.float32):\n",
    "    first_sample = next(iter(test_dataset))[\"input_ids\"]\n",
    "    first_input_ids = torch.tensor(first_sample, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    curr_len = first_input_ids.shape[1]\n",
    "    _, attention_mask, cos, sin, all_layer_kv_cache = qnn_llm_utils.prepare_inputs(first_input_ids)\n",
    "\n",
    "    attention_mask = attention_mask.to(data_type)\n",
    "    cos = cos.to(data_type)\n",
    "    sin = sin.to(data_type)\n",
    "    all_layer_kv_cache = [kv_cache.to(data_type) for kv_cache in all_layer_kv_cache]\n",
    "    \n",
    "    loss = 0\n",
    "    if num_batches == 0:\n",
    "        num_batches = num_total_batches\n",
    "    else:\n",
    "        num_batches = min(num_batches, num_total_batches)\n",
    "    for batch_id, batch in enumerate(tqdm(test_dataset, total=num_batches, desc=\"Evaluating PPL\")):\n",
    "        if batch_id >= num_batches:\n",
    "            break\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        curr_len = input_ids.shape[1]\n",
    "\n",
    "        input_ids = torch.cat(\n",
    "            [input_ids, torch.full((1, SEQ_LEN - curr_len), qnn_llm_utils.eos_token_id, device=input_ids.device)],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, cos, sin, attention_mask, all_layer_kv_cache)\n",
    "\n",
    "        lm_logits = output[0]\n",
    "        shift_logits = lm_logits[:, :-1, :][:, :curr_len, :].contiguous().to(dtype=torch.float32)\n",
    "        shift_labels = input_ids[:, 1:][:, :curr_len].contiguous().to(shift_logits.device)\n",
    "\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss += loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "    loss = loss / num_batches\n",
    "    ppl = loss.exp()\n",
    "    return ppl\n",
    "\n",
    "with event_marker(\"FP eval\"):\n",
    "    orig_ppl = ppl_eval(test_dataset, model, num_batches=20)\n",
    "print(f\"PPL: {orig_ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Model Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_data():\n",
    "    input_ids = next(iter(test_dataset))[\"input_ids\"]\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long, device=model.device).unsqueeze(0)\n",
    "    input_ids, attention_mask, cos, sin, all_layer_kv_cache = qnn_llm_utils.prepare_inputs(input_ids)\n",
    "    inputs_values = [input_ids, cos, sin, attention_mask]\n",
    "    inputs_values.extend(all_layer_kv_cache)\n",
    "    inputs_keys = ['input_ids', 'position_ids_cos', 'position_ids_sin','attention_mask'] \n",
    "    \n",
    "    kv_inputs_keys = []\n",
    "    for i in range(config.num_hidden_layers):\n",
    "        kv_inputs_keys.append(f\"past_key_{i}_in\")\n",
    "        kv_inputs_keys.append(f\"past_value_{i}_in\")\n",
    "    inputs_keys.extend(kv_inputs_keys)\n",
    "    inputs = dict(zip(inputs_keys, inputs_values))\n",
    "    return inputs, all_layer_kv_cache\n",
    "\n",
    "dummy_inputs, all_layer_kv_cache = get_dummy_data()\n",
    "len(dummy_inputs)\n",
    "\n",
    "# transverse and print all the key and value's shape, and dtype\n",
    "for key, value in dummy_inputs.items():\n",
    "    print(f\"{key}: {value.shape}, {value.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6. Prepare model using AIMET model preparer pro\n",
    "\n",
    "#### 6.1 KVCache MHA model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from aimet_torch.utils import load_pytorch_model\n",
    "import aimet_torch.pro.ir_graph_op_handler as ir_graph_op_handler\n",
    "from aimet_torch import onnx_utils\n",
    "from aimet_torch.pro import model_preparer\n",
    "# Setting this flag to False means that the prepared model will be flattened\n",
    "# This flag must be set to false because we rely on the model structure being flat to enable weight sharing\n",
    "onnx_utils.EXPORT_TO_ONNX_DIRECT = True\n",
    "ir_graph_op_handler.KEEP_ORIGINAL_MODEL_STRUCTURE = False\n",
    "\n",
    "from aimet_utils.rmsnorm_update import RmsNorm, RmsNormOphandler\n",
    "# Update ir graph op handler's registry with new RmsNormOpHandler class\n",
    "from aimet_torch.pro import ir_graph_op_handler\n",
    "ir_graph_op_handler.ir_to_handler_dict['RmsNorm'] = RmsNormOphandler\n",
    "\n",
    "# Register RmsNorm op definition in custom_modules_for_qnn\n",
    "from aimet_torch.pro import custom_modules_for_qnn\n",
    "setattr(custom_modules_for_qnn, 'RmsNorm', RmsNorm)\n",
    "\n",
    "\n",
    "\n",
    "dummy_input, all_layer_kv_cache = get_dummy_data()\n",
    "input_names = list(dummy_input.keys())\n",
    "output_names = ['logits'] \n",
    "for i in range(config.num_hidden_layers):\n",
    "    output_names.append(f'past_key_{i}_out')\n",
    "    output_names.append(f'past_value_{i}_out')\n",
    "\n",
    "# Build converter args\n",
    "converter_args_param = ['--input_layout']\n",
    "converter_args_value = 'NONTRIVIAL'\n",
    "converter_args = []\n",
    "for input_param in converter_args_param:\n",
    "    for input_name in input_names:\n",
    "        converter_args += [input_param, input_name, converter_args_value]\n",
    "\n",
    "skip_prepare = False # This is done only once\n",
    "prepare_path = os.path.join(output_dir, 'prepare')\n",
    "os.makedirs(prepare_path, exist_ok=True)\n",
    "prepare_filename = f'{model_name}_kvcache_{config.num_hidden_layers}_layer'\n",
    "\n",
    "\n",
    "if skip_prepare:\n",
    "    with event_marker(f\"KVCache load pre-prepared {prepare_filename}\", flush_ram=True):\n",
    "        prepared_model_path = os.path.join(prepare_path, f'{prepare_filename}.py')\n",
    "        if not os.path.exists(prepared_model_path):\n",
    "            raise ValueError(f\"prepared artifacts not found in {prepare_path}\")\n",
    "        else:\n",
    "            print(f'WARNING: preparation skipped for model={prepare_filename}, prepared at {time.ctime(os.path.getmtime(prepared_model_path))}')\n",
    "            prepared_model = load_pytorch_model(path=prepare_path, filename=prepare_filename,\n",
    "                                                model_name=prepare_filename, load_state_dict=True)\n",
    "\n",
    "else:\n",
    "    with event_marker(\"KVCache prepare model\", flush_ram=True):\n",
    "        if(enable_fp16):\n",
    "            convert_model_to_fp32(model)\n",
    "        dummy_input_for_prepare = {\n",
    "            \"input_ids\": dummy_inputs[\"input_ids\"],\n",
    "            \"cos\": dummy_inputs[\"position_ids_cos\"],\n",
    "            \"sin\": dummy_inputs[\"position_ids_sin\"],\n",
    "            \"attention_mask\": dummy_inputs[\"attention_mask\"],\n",
    "            \"all_layers_kv_cache\": all_layer_kv_cache\n",
    "        }\n",
    "        prepared_model = model_preparer.prepare_model(model,\n",
    "                                                      dummy_input_for_prepare,\n",
    "                                                      model_name=prepare_filename,\n",
    "                                                      filename=prepare_filename,\n",
    "                                                      path=prepare_path,\n",
    "                                                      input_names=input_names,\n",
    "                                                      output_names=output_names,\n",
    "                                                      onnx_export_args={\"opset_version\":14},\n",
    "                                                      converter_args=converter_args)\n",
    "del model # original model no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Convert prepared model to FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(enable_fp16):\n",
    "   convert_model_to_fp16(prepared_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Model prepare verification\n",
    "Verify if prepared KV cache model generates the same PPL as FP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_model.to(device)\n",
    "\n",
    "def ppl_eval(test_dataset, model, num_batches=0, data_type=torch.float32):\n",
    "    first_sample = next(iter(test_dataset))[\"input_ids\"]\n",
    "    first_input_ids = torch.tensor(first_sample, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    curr_len = first_input_ids.shape[1]\n",
    "    _, attention_mask, cos, sin, all_layer_kv_cache = qnn_llm_utils.prepare_inputs(first_input_ids)\n",
    "\n",
    "    attention_mask = attention_mask.to(data_type)\n",
    "    cos = cos.to(data_type)\n",
    "    sin = sin.to(data_type)\n",
    "    all_layer_kv_cache = [kv_cache.to(data_type) for kv_cache in all_layer_kv_cache]\n",
    "    \n",
    "    loss = 0\n",
    "    if num_batches == 0:\n",
    "        num_batches = num_total_batches\n",
    "    else:\n",
    "        num_batches = min(num_batches, num_total_batches)\n",
    "    for batch_id, batch in enumerate(tqdm(test_dataset, total=num_batches, desc=\"Evaluating PPL\")):\n",
    "        if batch_id >= num_batches:\n",
    "            break\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        curr_len = input_ids.shape[1]\n",
    "\n",
    "        input_ids = torch.cat(\n",
    "            [input_ids, torch.full((1, SEQ_LEN - curr_len), qnn_llm_utils.eos_token_id, device=input_ids.device)],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            all_inputs = (input_ids, cos, sin, attention_mask, *all_layer_kv_cache)\n",
    "            output = model(*all_inputs)\n",
    "\n",
    "        lm_logits = output[0]\n",
    "        shift_logits = lm_logits[:, :-1, :][:, :curr_len, :].contiguous().to(dtype=torch.float32)\n",
    "        shift_labels = input_ids[:, 1:][:, :curr_len].contiguous().to(shift_logits.device)\n",
    "\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss += loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "    loss = loss / num_batches\n",
    "    ppl = loss.exp()\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with event_marker(\"KVcache prepared FP eval\", flush_ram=True):\n",
    "    with torch.no_grad():\n",
    "        prepared_kvcache_ppl = ppl_eval(test_dataset, prepared_model, num_batches=20)\n",
    "print(f\"ppl score of KVCACHE prepared fp model: {prepared_kvcache_ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Quantization\n",
    "\n",
    "The _Quantization_ step is the primary focus of this notebook, this section could be modified to execute various quantization experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8.1 Create quantsim configured for QNN HTP target \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_torch.v2.quantsim import QuantizationSimModel\n",
    "import copy\n",
    "\n",
    "sim_model = copy.deepcopy(prepared_model)\n",
    "sim_model.to(device)\n",
    "\n",
    "def get_dummy_data():\n",
    "    input_ids = next(iter(test_dataset))[\"input_ids\"]\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long, device=\"cuda\").unsqueeze(0)\n",
    "    input_ids, attention_mask, cos, sin, all_layer_kv_cache = qnn_llm_utils.prepare_inputs(input_ids)\n",
    "    inputs_values = [input_ids, cos, sin, attention_mask]\n",
    "    inputs_values.extend(all_layer_kv_cache)\n",
    "    inputs_keys = ['input_ids', 'position_ids_cos', 'position_ids_sin','attention_mask'] \n",
    "    \n",
    "    kv_inputs_keys = []\n",
    "    for i in range(config.num_hidden_layers):\n",
    "        kv_inputs_keys.append(f\"past_key_{i}_in\")\n",
    "        kv_inputs_keys.append(f\"past_value_{i}_in\")\n",
    "    inputs_keys.extend(kv_inputs_keys)\n",
    "    inputs = dict(zip(inputs_keys, inputs_values))\n",
    "    return inputs, all_layer_kv_cache\n",
    "\n",
    "dummy_input, _ = get_dummy_data()\n",
    "dummy_input_for_quantsim = tuple(dummy_input.values())\n",
    "with event_marker(\"create KVCache Quantsim\"):\n",
    "    quantsim = QuantizationSimModel(model=sim_model,\n",
    "                                    quant_scheme=QuantScheme.post_training_tf,\n",
    "                                    dummy_input=dummy_input_for_quantsim,\n",
    "                                    default_output_bw=16,\n",
    "                                    default_param_bw=8,\n",
    "                                    in_place=True,\n",
    "                                    config_file=htp_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8.2 Setting 16bit x 8bit matmuls\n",
    "To keep key and value tensors as 8 bits, reducing data I/O costs associated with KV-cache orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.v2.experimental.quantsim_utils import set_matmul_second_input_producer_to_8bit_symmetric\n",
    "\n",
    "set_matmul_second_input_producer_to_8bit_symmetric(quantsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8.3 Concat encoding unification\n",
    "configuring concat ops to have shared encoding on input and output activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.v2.experimental import propagate_output_encodings\n",
    "import aimet_torch.elementwise_ops as aimet_ops\n",
    "propagate_output_encodings(quantsim, aimet_ops.Concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### 8.4 Manual Mixed Precision\n",
    "applying mixed precision configuration to ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils.mixed_precision_overrides import ManualQuantsimMixedPrecisionConfig\n",
    "quantsim_adjuster = ManualQuantsimMixedPrecisionConfig(mixed_precision_config_file= \"./config/mixed_precision_config/exceptions.json\")\n",
    "quantsim_adjuster.apply_exceptions(quantsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8.5 Sequential MSE\n",
    "applying sequential MSE technique to optimize parameter encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.v2.seq_mse import apply_seq_mse\n",
    "from aimet_torch.seq_mse import SeqMseParams\n",
    "from aimet_torch.utils import load_pytorch_model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = load_dataset(\"nexa4ai/qwen3_wiki_calib\", split=\"train\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "input_ids = next(iter(train_dataloader))[\"input_ids\"]\n",
    "input_ids = torch.tensor(input_ids, dtype=torch.long, device=\"cuda\").unsqueeze(0)\n",
    "input_ids, attention_mask, cos, sin, all_layer_kv_cache = qnn_llm_utils.prepare_inputs(input_ids)\n",
    "\n",
    "def _forward_fn(model, inputs):\n",
    "    # slice inputs so that we only end up doing inference using first n tokens\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long, device=\"cuda\").unsqueeze(0)\n",
    "    curr_len = input_ids.shape[1]\n",
    "    input_ids = torch.cat(\n",
    "        [input_ids, torch.full((1, 2073 - curr_len), 151645, device=input_ids.device)],\n",
    "        dim=-1,\n",
    "    )\n",
    "    all_inputs = (input_ids, cos, sin, attention_mask, *all_layer_kv_cache)\n",
    "    model(*all_inputs)\n",
    "\n",
    "\n",
    "## HACK: We change num_batches from 20 to 1 to save time during the learning stage.\n",
    "params = SeqMseParams(num_batches=20,\n",
    "                      inp_symmetry=\"symqt\",\n",
    "                      num_candidates=20,\n",
    "                      loss_fn=\"mse\",\n",
    "                      forward_fn=_forward_fn)\n",
    "\n",
    "\n",
    "with event_marker(\"SeqMSE\"):\n",
    "    prepared_model.to(\"cuda\")\n",
    "    quantsim.model.to(\"cuda\")\n",
    "    apply_seq_mse(prepared_model, quantsim, train_dataloader, params)\n",
    "\n",
    "del prepared_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "---\n",
    "### 8.6 Calibration\n",
    "compute activation encodings using AIMET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataset = load_dataset(\"nexa4ai/qwen25_wiki_calib\", split=\"train\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "input_ids = next(iter(train_dataloader))[\"input_ids\"]\n",
    "input_ids = torch.tensor(input_ids, dtype=torch.long, device=\"cuda\").unsqueeze(0)\n",
    "input_ids, attention_mask, cos, sin, all_layer_kv_cache = qnn_llm_utils.prepare_inputs(input_ids)\n",
    "\n",
    "def _forward_fn(model, kwargs):\n",
    "    data_loader = kwargs['data_loader']\n",
    "    max_iterations = kwargs['num_batches']\n",
    "    \n",
    "    for batch_id, batch in enumerate(tqdm(data_loader, total=max_iterations)):\n",
    "        if batch_id < max_iterations:\n",
    "            input_ids = batch['input_ids']\n",
    "            input_ids = torch.tensor(input_ids, dtype=torch.long, device=\"cuda\").unsqueeze(0)\n",
    "            curr_len = input_ids.shape[1]\n",
    "            input_ids = torch.cat(\n",
    "                [input_ids, torch.full((1, 2073 - curr_len), 151645, device=input_ids.device)],\n",
    "                dim=-1,\n",
    "            )\n",
    "            all_inputs = (input_ids, cos, sin, attention_mask, *all_layer_kv_cache)\n",
    "            model(*all_inputs)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "kwargs = {\n",
    "   'data_loader': train_dataloader,\n",
    "   'num_batches': 20\n",
    "}\n",
    "\n",
    "with event_marker(\"compute encoding\", flush_ram=True):\n",
    "    quantsim.model.to(\"cuda\")\n",
    "    quantsim.compute_encodings(_forward_fn, kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7 Eval KV Cache sim model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with event_marker(\"KV cache sim eval\", flush_ram=True):\n",
    "    with torch.no_grad():\n",
    "        quantsim.model.to(\"cuda\")\n",
    "        sim_ppl = ppl_eval(test_dataset, quantsim.model, num_batches=20)\n",
    "\n",
    "print(f\"ppl score of KVCACHE sim fp model: {sim_ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.8 Real Test\n",
    "\n",
    "We conduct the real LLM inference process to ensure that the quantized model can do the reasonable inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = 151645\n",
    "eos_tokens = {151645}\n",
    "input_text = (\n",
    "    \"<|im_start|>user\\nIntroduce Newton's first law of motion. Be short and concise.<|im_end|>\"\n",
    "    \"\\n<|im_start|>assistant\\n\"\n",
    ")\n",
    "input_ids_list = tokenizer.encode(input_text)  # a list of ints\n",
    "\n",
    "n_past = 0\n",
    "curr_len = len(input_ids_list)\n",
    "\n",
    "if curr_len < SEQ_LEN:\n",
    "    input_ids_list = input_ids_list + [eos_token_id] * (SEQ_LEN - curr_len)\n",
    "\n",
    "input_ids = torch.tensor(input_ids_list, dtype=torch.long, device=device).unsqueeze(0)\n",
    "attention_mask = qnn_llm_utils.get_attention_mask(n_past, curr_len)\n",
    "position_ids = qnn_llm_utils.get_position_ids(n_past, SEQ_LEN)\n",
    "cos, sin = qnn_llm_utils.get_cos_sin(attention_mask, position_ids)  # attn_mask as dummpy input to get device\n",
    "all_layer_kv_caches = qnn_llm_utils.get_kv_cache()\n",
    "last_token_indices = torch.tensor([n_past + curr_len - 1], dtype=torch.long, device=device)\n",
    "\n",
    "generated_ids = []\n",
    "\n",
    "for i in range(10):\n",
    "    with torch.no_grad():\n",
    "        prepared_inputs = (input_ids, cos, sin, attention_mask, *all_layer_kv_caches)\n",
    "        outputs = quantsim.model(*prepared_inputs)\n",
    "        next_token_logits = outputs[0][:, last_token_indices, :]\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1).item()\n",
    "        generated_ids.append(next_token_id)\n",
    "\n",
    "        ## update the inputs for next token\n",
    "        all_layer_kv_caches = qnn_llm_utils.update_kv_cache(all_layer_kv_caches, outputs[1:], n_past, curr_len)\n",
    "        n_past += curr_len\n",
    "        curr_len = 1\n",
    "        next_input_ids = [next_token_id] + [eos_token_id] * (SEQ_LEN - 1)\n",
    "        input_ids = torch.tensor(next_input_ids, dtype=torch.long, device=device).unsqueeze(\n",
    "            0\n",
    "        )  # set bs_size = 1 manually\n",
    "        last_token_indices = torch.tensor([curr_len - 1], dtype=torch.long, device=device)\n",
    "        attention_mask = qnn_llm_utils.get_attention_mask(n_past, curr_len)\n",
    "        position_ids = qnn_llm_utils.get_position_ids(n_past, SEQ_LEN)\n",
    "        cos, sin = qnn_llm_utils.get_cos_sin(attention_mask, position_ids)\n",
    "\n",
    "    if next_token_id in eos_tokens:\n",
    "        break\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Export\n",
    "the pipeline call below would export onnx model, encoding and test vector for KVCache models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 9.1 Generating test vectors for QNN SDK\n",
    "\n",
    "We actually only use one piece of data. Thus, the computation can be done in CPU, since this step can be easily OOM on CUDA.\n",
    "\n",
    "\n",
    "<span style=\"color:#d62728;font-weight:bold;\">TODO</span>: We still need to figure out what this is, and how we are going to use it. We didn't run through this cell for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from llm_utils.test_vectors import generate_test_vectors\n",
    "# del prepared_model  \n",
    "test_vector_layers = [\n",
    "    \"model_layers_\\\\d+_input_layernorm_Pow\",\n",
    "    \"lm_head_conv_Conv\"\n",
    "]\n",
    "with event_marker(\"generate test vector\"):\n",
    "    quantsim.model.to(\"cpu\")\n",
    "    generate_test_vectors(quantsim, qnn_llm_utils, train_dataloader, output_dir, \n",
    "                          num_batches=1, test_vector_layers=test_vector_layers, input_names=input_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 9.2 Export KVCache Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input, _ = get_dummy_data()\n",
    "dummy_input = tuple(dummy_input.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.utils import change_tensor_device_placement\n",
    "from aimet_torch.onnx_utils import OnnxExportApiArgs\n",
    "onnx_dir = os.path.join(output_dir, 'onnx')\n",
    "os.makedirs(onnx_dir, exist_ok=True)\n",
    "\n",
    "if(enable_fp16):\n",
    "    # Convert FP16 model back to FP32 for ONNX export\n",
    "    convert_model_to_fp32(quantsim.model)\n",
    "\n",
    "quantsim.model.to(\"cpu\")\n",
    "onnx_api_args = OnnxExportApiArgs(input_names=input_names,output_names=output_names, opset_version=14)\n",
    "sample_inputs = change_tensor_device_placement(dummy_input, torch.device('cpu'))\n",
    "with event_marker(\"KVCache export\", flush_ram=True):\n",
    "    quantsim.export(onnx_dir, model_name, sample_inputs, onnx_export_args=onnx_api_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "from aimet_torch.pro.utils.profiler import EventProfiler\n",
    "EventProfiler().report()\n",
    "EventProfiler().json_dump(os.path.join(output_dir, 'profiling_stats'))\n",
    "\n",
    "import json\n",
    "with open(f'{output_dir}/ppl.json', 'wt') as f:\n",
    "    json.dump({\n",
    "        \"original\": float(orig_ppl),\n",
    "        \"prepared_kvcache\": float(prepared_kvcache_ppl),\n",
    "        \"QuantSim\": float(sim_ppl),\n",
    "    }, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Qualcomm Technologies, Inc. and/or its subsidiaries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
