{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# AIMET Quantization workflow for Llama 3.2 3B Context Length 4K\n",
            "\n",
            "This notebook shows a working code example of how to use AIMET to quantize LlamaV3.2 model.\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---\n",
            "\n",
            "### Required packages\n",
            "\n",
            "The notebook assumes AIMET and LLamaV3.2 related packages are already installed.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Install packages only if running in jupyter notebook mode\n",
            "# if hasattr(__builtins__,'__IPYTHON__'):\n",
            "#     !sudo -H apt-get -qq update\n",
            "#     !sudo -H apt-get -qq install libc++-dev\n",
            "#     !sudo -H pip install --quiet --upgrade --root-user-action=ignore --no-cache-dir transformers==4.43.2\n",
            "#     !sudo -H pip install --quiet --upgrade --root-user-action=ignore --no-cache-dir tokenizers==0.19.0"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Overall flow\n",
            "\n",
            "This notebook covers the following\n",
            "\n",
            "1. Setting QNN SDK\n",
            "2. Instantiate and adapt FP32 model\n",
            "3. Complete the last step(s) of model adaptation\n",
            "4. Convert FP32 model to FP16\n",
            "5. Model Evaluation\n",
            "6. Model Sample Input\n",
            "7. Prepare model using AIMET model preparer pro\n",
            "8. Quantization\n",
            "9. Export\n",
            "\n",
            "### What this notebook is not\n",
            "\n",
            "- This notebook is not intended to show the full scope of optimization. For example, the flow will not use QAT, KD-QAT as deliberate choice to have the notebook execute more quickly.\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 1.1 Setting QNN SDK\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "sh: 1: source: not found\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[INFO] AISW SDK environment set\n",
                  "[INFO] QNN_SDK_ROOT: /opt/qcom/aistack/qairt/2.34.2.250528\n",
                  "[INFO] SNPE_ROOT: /opt/qcom/aistack/qairt/2.34.2.250528\n"
               ]
            }
         ],
         "source": [
            "import sys\n",
            "import os\n",
            "QNN_SDK_ROOT='/opt/qcom/aistack/qairt/2.34.2.250528/' # QNN 2.33\n",
            "os.environ['QNN_SDK_ROOT'] = QNN_SDK_ROOT\n",
            "os.system(f'source {QNN_SDK_ROOT}/bin/envsetup.sh')\n",
            "\n",
            "!source {QNN_SDK_ROOT}/bin/envsetup.sh\n",
            "\n",
            "assert QNN_SDK_ROOT != '', 'Please point the QNN_SDK_ROOT variable to your QNN SDK'\n",
            "lib_clang_path = os.path.join(QNN_SDK_ROOT, 'lib', 'x86_64-linux-clang')\n",
            "sys.path.insert(0, QNN_SDK_ROOT + '/lib/python')\n",
            "LD_LIBRARY_PATH = os.getenv('LD_LIBRARY_PATH', None)\n",
            "os.environ['LD_LIBRARY_PATH'] = lib_clang_path + ':' + LD_LIBRARY_PATH if LD_LIBRARY_PATH is not None else lib_clang_path\n",
            "enable_fp16 = False # Flag to enable e2e fp16 flow, set to false to set fp32 flow"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 1.2 Setting NSP Target\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "/home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/htp_quantsim_config_v73.json\n"
               ]
            }
         ],
         "source": [
            "sys.path.append('../../common/')\n",
            "from utilities.nsptargets import NspTargets\n",
            "\n",
            "# Windows GEN 2 is supported for this notebook\n",
            "nsp_target = NspTargets.Windows.GEN2\n",
            "\n",
            "## Select quantsim config based on target\n",
            "## HACK: We should consider to change this as some fixed config in the future\n",
            "htp_config_file = f'/home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/htp_quantsim_config_{nsp_target.dsp_arch}.json'\n",
            "print(htp_config_file)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---\n",
            "\n",
            "### 2. Instantiate and adapt FP32 model\n",
            "\n",
            "Now, we will use our own unique implementation for the Qwen3 model. Use our own implementation to instantiate the FP32 model.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {
            "collapsed": false,
            "jupyter": {
               "outputs_hidden": false
            },
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/home/azureuser/.conda/envs/aimet/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                  "  from .autonotebook import tqdm as notebook_tqdm\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "num_layer: 1,  num_hidden_size :16,  num_kv_heads: 8\n"
               ]
            }
         ],
         "source": [
            "import os\n",
            "import sys\n",
            "\n",
            "os.environ['HF_TOKEN'] = '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'\n",
            "import torch\n",
            "from hf_tokenizers import Tokenizer\n",
            "sys.path.append(\"../../../qwen3_torch\")\n",
            "from modeling_qwen3 import QNNQwen3, QNNLLMUtils\n",
            "\n",
            "model_name = 'qwen3'\n",
            "model_id = 'Qwen/Qwen3-0.6B'\n",
            "\n",
            "cache_dir = './cache_dir'\n",
            "output_dir = f'./output_dir_{os.path.basename(model_id)}'\n",
            "os.makedirs(output_dir, exist_ok=True)\n",
            "\n",
            "qnn_model = QNNQwen3.from_pretrained(model_id, cache_dir=cache_dir)\n",
            "tokenizer = Tokenizer(\"/home/azureuser/zack/qnn-expr/experiments/qwen3_tokenizer.json\")\n",
            "\n",
            "\n",
            "# ==== restrict to use only 1 layer ====\n",
            "qnn_model.model.layers = qnn_model.model.layers[:1]\n",
            "qnn_model.config.num_hidden_layers = 1\n",
            "config = qnn_model.config\n",
            "print(f'num_layer: {config.num_hidden_layers}'\n",
            "      f',  num_hidden_size :{config.num_attention_heads},  num_kv_heads: {config.num_key_value_heads}')\n",
            "# ======================================\n",
            "\n",
            "qnn_model.qnn_init()\n",
            "config = qnn_model.config\n",
            "SEQ_LEN = 2073\n",
            "KV_LEN = 4096 - SEQ_LEN\n",
            "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
            "qnn_model.to(device)\n",
            "qnn_llm_utils = QNNLLMUtils(SEQ_LEN, KV_LEN, device, config)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "QNNQwen3(\n",
                     "  (model): Module(\n",
                     "    (embed_tokens): Embedding(151936, 1024)\n",
                     "    (layers): ModuleList(\n",
                     "      (0): QNNQwen3DecoderLayer(\n",
                     "        (self_attn): QNNQwen3Attention(\n",
                     "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
                     "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
                     "          (q_proj_conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                     "          (k_proj_conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                     "          (v_proj_conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                     "          (o_proj_conv): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                     "        )\n",
                     "        (mlp): QNNQwen3MLP(\n",
                     "          (act_fn): SiLU()\n",
                     "          (gate_proj_conv): Conv2d(1024, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                     "          (up_proj_conv): Conv2d(1024, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                     "          (down_proj_conv): Conv2d(3072, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                     "        )\n",
                     "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
                     "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
                     "      )\n",
                     "    )\n",
                     "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
                     "    (lm_head_conv): Conv2d(1024, 151936, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                     "  )\n",
                     ")"
                  ]
               },
               "execution_count": 5,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "qnn_model"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 2.1 Direct verification\n",
            "\n",
            "We use the models loaded, and directly check if the model can do correct inference. Ensure the model can create the reasonable outputs.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[nodeelefaultsfulsivenessivenessivenessiveness\n"
               ]
            }
         ],
         "source": [
            "eos_token_id = 151645\n",
            "eos_tokens = {151645}\n",
            "input_text = (\n",
            "    \"<|im_start|>user\\nIntroduce Newton's first law of motion. Be short and concise.<|im_end|>\"\n",
            "    \"\\n<|im_start|>assistant\\n\"\n",
            ")\n",
            "input_ids_list = tokenizer.encode(input_text)  # a list of ints\n",
            "\n",
            "n_past = 0\n",
            "curr_len = len(input_ids_list)\n",
            "\n",
            "if curr_len < SEQ_LEN:\n",
            "    input_ids_list = input_ids_list + [eos_token_id] * (SEQ_LEN - curr_len)\n",
            "\n",
            "input_ids = torch.tensor(input_ids_list, dtype=torch.long, device=device).unsqueeze(0)\n",
            "attention_mask = qnn_llm_utils.get_attention_mask(n_past, curr_len)\n",
            "position_ids = qnn_llm_utils.get_position_ids(n_past, SEQ_LEN)\n",
            "cos, sin = qnn_llm_utils.get_cos_sin(attention_mask, position_ids)  # attn_mask as dummpy input to get device\n",
            "all_layer_kv_caches = qnn_llm_utils.get_kv_cache()\n",
            "last_token_indices = torch.tensor([n_past + curr_len - 1], dtype=torch.long, device=device)\n",
            "\n",
            "generated_ids = []\n",
            "\n",
            "for i in range(10):\n",
            "    with torch.no_grad():\n",
            "\n",
            "        outputs = qnn_model(input_ids, cos, sin, attention_mask, all_layer_kv_caches)\n",
            "        next_token_logits = outputs[0][:, last_token_indices, :]\n",
            "        next_token_id = torch.argmax(next_token_logits, dim=-1).item()\n",
            "        generated_ids.append(next_token_id)\n",
            "\n",
            "        ## update the inputs for next token\n",
            "        all_layer_kv_caches = qnn_llm_utils.update_kv_cache(all_layer_kv_caches, outputs[1:], n_past, curr_len)\n",
            "        n_past += curr_len\n",
            "        curr_len = 1\n",
            "        next_input_ids = [next_token_id] + [eos_token_id] * (SEQ_LEN - 1)\n",
            "        input_ids = torch.tensor(next_input_ids, dtype=torch.long, device=device).unsqueeze(\n",
            "            0\n",
            "        )  # set bs_size = 1 manually\n",
            "        last_token_indices = torch.tensor([curr_len - 1], dtype=torch.long, device=device)\n",
            "        attention_mask = qnn_llm_utils.get_attention_mask(n_past, curr_len)\n",
            "        position_ids = qnn_llm_utils.get_position_ids(n_past, SEQ_LEN)\n",
            "        cos, sin = qnn_llm_utils.get_cos_sin(attention_mask, position_ids)\n",
            "\n",
            "    if next_token_id in eos_tokens:\n",
            "        break\n",
            "\n",
            "generated_text = tokenizer.decode(generated_ids)\n",
            "print(generated_text)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---\n",
            "\n",
            "### 3. Convert FP32 model to FP16\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "The following code contains a temporary measure needed to maintain model accuracy when converting to FP16. RMSnorm operators are very sensitive to changes in bitwidth, and must upcast the input tensor to FP32 first. Once the QNN converter is able to recognize and coalesce RMSnorm operations, this upconversion will be handled automatically. Until then, we must insert operators to upcast tensors to FP32 before the first RMSnorm component operation, and downcast tensors back to FP16 once all RMSnorm component ops are complete.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:29:11,197 - root - INFO - aimetpro-release-1.34.0_Build_Id_0.207.0.44.torch-gpu-pt113-release\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/tmp/ipykernel_3666497/2303934292.py:1: UserWarning: \u001b[31;21m\"aimet_torch.elementwise_ops\" is renamed to \"aimet_torch.nn.modules.custom\" and will be deprecated soon in the later versions.\u001b[0m\n",
                  "  from aimet_torch import elementwise_ops\n"
               ]
            }
         ],
         "source": [
            "from aimet_torch import elementwise_ops\n",
            "\n",
            "class PreCast(torch.nn.Module):\n",
            "    def __init__(self, module, dtype):\n",
            "        super(PreCast, self).__init__()\n",
            "        self.module = module\n",
            "        self.upcast = elementwise_ops.Cast(dtype)\n",
            "\n",
            "    def forward(self, *inputs):\n",
            "        casted_inputs = [self.upcast(input) for input in inputs]\n",
            "        return self.module(*casted_inputs)\n",
            "\n",
            "class PostCast(torch.nn.Module):\n",
            "    def __init__(self, module, dtype):\n",
            "        super(PostCast, self).__init__()\n",
            "        self.module = module\n",
            "        self.downcast = elementwise_ops.Cast(dtype)\n",
            "\n",
            "    def forward(self, *inputs):\n",
            "        output = self.module(*inputs)\n",
            "        casted_output = self.downcast(output)\n",
            "        return casted_output\n",
            "    \n",
            "# Helper function to convert FP32 model to FP16\n",
            "# Inserts upcast and downcast operators around RMSnorm operators if found in the graph\n",
            "## NEXA: After model preparation, rms norm will have two ops norm_Pow and norm_Mul_1,\n",
            "#        This is why we are using the norm_Pow, and norm_Mul_1.\n",
            "def convert_model_to_fp16(model):\n",
            "    model.half()\n",
            "    for name, module in model.named_modules():\n",
            "        if name.endswith(\"norm_Pow\"):\n",
            "            setattr(model, name, PreCast(module, torch.float32))\n",
            "        if name.endswith(\"norm_Mul_1\"):\n",
            "            setattr(model, name, PostCast(module, torch.float16))\n",
            "            \n",
            "# Helper function to convert FP16 model back to FP32\n",
            "# Removes upcast and downcast operators inserted by convert_model_to_fp16, if present\n",
            "def convert_model_to_fp32(model):\n",
            "    model.float()\n",
            "    \n",
            "    for name, module in model.named_modules():\n",
            "        if name.endswith(\"norm_Pow\"):\n",
            "            setattr(model, name, module.module)\n",
            "        if name.endswith(\"norm_Mul_1\"):\n",
            "            setattr(model, name, module.module)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "model = qnn_model\n",
            "if(enable_fp16):\n",
            "   convert_model_to_fp16(model)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---\n",
            "\n",
            "### 4. Model Evaluation\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:29:11,505 - datasets - INFO - PyTorch version 1.13.1+cu117 available.\n",
                  "2025-07-26 16:29:14,005 - Utils - INFO - Created RAM watermark daemon process(pid=3666636) for pid=3666497, polling at 100.0 ms\n",
                  "2025-07-26 16:29:14,004 - Utils - INFO - Created Latency/Memory profiler: empty_cache=False\n",
                  "2025-07-26 16:29:14,012 - Utils - INFO - memory usage @ 'FP eval >> ' : GPU default:2.5 GB, RAM 3.1 GB\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Evaluating PPL: 100%|██████████| 20/20 [00:00<00:00, 37.17it/s]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:29:21,080 - Utils - INFO - memory usage @ 'FP eval << ' : GPU default:7.8 GB, RAM 3.1 GB\n",
                  "2025-07-26 16:29:21,081 - Utils - INFO - Event FP eval : time=0:00:07, GPU=5.4 GB(+ 2.5 GB), RAM=33.0 MB(+ 3.1 GB)\n",
                  "PPL: 2703437.75\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "\n"
               ]
            }
         ],
         "source": [
            "from torch.nn import CrossEntropyLoss\n",
            "from tqdm import tqdm\n",
            "from datasets import load_dataset\n",
            "from aimet_torch.pro.utils.profiler import event_marker\n",
            "\n",
            "test_dataset = load_dataset(\"nexa4ai/qwen3_wiki_calib\", split=\"test\")\n",
            "num_total_batches = len(test_dataset)\n",
            "\n",
            "\n",
            "def ppl_eval(test_dataset, model, num_batches=0, data_type=torch.float32):\n",
            "    first_sample = next(iter(test_dataset))[\"input_ids\"]\n",
            "    first_input_ids = torch.tensor(first_sample, dtype=torch.long, device=device).unsqueeze(0)\n",
            "    curr_len = first_input_ids.shape[1]\n",
            "    _, attention_mask, cos, sin, all_layer_kv_cache = qnn_llm_utils.prepare_inputs(first_input_ids)\n",
            "\n",
            "    attention_mask = attention_mask.to(data_type)\n",
            "    cos = cos.to(data_type)\n",
            "    sin = sin.to(data_type)\n",
            "    all_layer_kv_cache = [kv_cache.to(data_type) for kv_cache in all_layer_kv_cache]\n",
            "    \n",
            "    loss = 0\n",
            "    if num_batches == 0:\n",
            "        num_batches = num_total_batches\n",
            "    else:\n",
            "        num_batches = min(num_batches, num_total_batches)\n",
            "    for batch_id, batch in enumerate(tqdm(test_dataset, total=num_batches, desc=\"Evaluating PPL\")):\n",
            "        if batch_id >= num_batches:\n",
            "            break\n",
            "\n",
            "        input_ids = batch[\"input_ids\"]\n",
            "        input_ids = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
            "        curr_len = input_ids.shape[1]\n",
            "\n",
            "        input_ids = torch.cat(\n",
            "            [input_ids, torch.full((1, SEQ_LEN - curr_len), qnn_llm_utils.eos_token_id, device=input_ids.device)],\n",
            "            dim=-1,\n",
            "        )\n",
            "\n",
            "        with torch.no_grad():\n",
            "            output = model(input_ids, cos, sin, attention_mask, all_layer_kv_cache)\n",
            "\n",
            "        lm_logits = output[0]\n",
            "        shift_logits = lm_logits[:, :-1, :][:, :curr_len, :].contiguous().to(dtype=torch.float32)\n",
            "        shift_labels = input_ids[:, 1:][:, :curr_len].contiguous().to(shift_logits.device)\n",
            "\n",
            "        loss_fct = CrossEntropyLoss()\n",
            "        loss += loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
            "\n",
            "    loss = loss / num_batches\n",
            "    ppl = loss.exp()\n",
            "    return ppl\n",
            "\n",
            "with event_marker(\"FP eval\"):\n",
            "    orig_ppl = ppl_eval(test_dataset, model, num_batches=20)\n",
            "print(f\"PPL: {orig_ppl}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---\n",
            "\n",
            "### 5. Model Sample Input\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "input_ids: torch.Size([1, 2073]), torch.int64\n",
                  "position_ids_cos: torch.Size([1, 1, 2073, 64]), torch.float32\n",
                  "position_ids_sin: torch.Size([1, 1, 2073, 64]), torch.float32\n",
                  "attention_mask: torch.Size([1, 1, 2073, 4096]), torch.float32\n",
                  "past_key_0_in: torch.Size([1, 8, 128, 2023]), torch.float32\n",
                  "past_value_0_in: torch.Size([1, 8, 2023, 128]), torch.float32\n"
               ]
            }
         ],
         "source": [
            "def get_dummy_data():\n",
            "    input_ids = next(iter(test_dataset))[\"input_ids\"]\n",
            "    input_ids = torch.tensor(input_ids, dtype=torch.long, device=model.device).unsqueeze(0)\n",
            "    input_ids, attention_mask, cos, sin, all_layer_kv_cache = qnn_llm_utils.prepare_inputs(input_ids)\n",
            "    inputs_values = [input_ids, cos, sin, attention_mask]\n",
            "    inputs_values.extend(all_layer_kv_cache)\n",
            "    inputs_keys = ['input_ids', 'position_ids_cos', 'position_ids_sin','attention_mask'] \n",
            "    \n",
            "    kv_inputs_keys = []\n",
            "    for i in range(config.num_hidden_layers):\n",
            "        kv_inputs_keys.append(f\"past_key_{i}_in\")\n",
            "        kv_inputs_keys.append(f\"past_value_{i}_in\")\n",
            "    inputs_keys.extend(kv_inputs_keys)\n",
            "    inputs = dict(zip(inputs_keys, inputs_values))\n",
            "    return inputs, all_layer_kv_cache\n",
            "\n",
            "dummy_inputs, all_layer_kv_cache = get_dummy_data()\n",
            "len(dummy_inputs)\n",
            "\n",
            "# transverse and print all the key and value's shape, and dtype\n",
            "for key, value in dummy_inputs.items():\n",
            "    print(f\"{key}: {value.shape}, {value.dtype}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---\n",
            "\n",
            "### 6. Prepare model using AIMET model preparer pro\n",
            "\n",
            "#### 6.1 KVCache MHA model preparation\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/home/azureuser/.conda/envs/aimet/lib/python3.10/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
                  "  param_schemas = callee.param_schemas()\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:29:36,613 - Utils - INFO - memory usage @ 'KVCache prepare model[gc] >> ' : GPU default:2.6 GB, RAM 3.2 GB\n",
                  "2025-07-26 16:29:44,565 - root - INFO - Input shape info \n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:29:44,565 - 270 - INFO - Input shape info \n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:30:26,637 - Utils - INFO - memory usage @ 'KVCache prepare model[gc] << ' : GPU default:7.6 GB, RAM 8.1 GB\n",
                  "2025-07-26 16:30:26,638 - Utils - INFO - Event KVCache prepare model[gc] : time=0:00:50, GPU=5.1 GB(+ 2.6 GB), RAM=4.9 GB(+ 3.2 GB)\n"
               ]
            }
         ],
         "source": [
            "import time\n",
            "\n",
            "from aimet_torch.utils import load_pytorch_model\n",
            "import aimet_torch.pro.ir_graph_op_handler as ir_graph_op_handler\n",
            "from aimet_torch import onnx_utils\n",
            "from aimet_torch.pro import model_preparer\n",
            "# Setting this flag to False means that the prepared model will be flattened\n",
            "# This flag must be set to false because we rely on the model structure being flat to enable weight sharing\n",
            "onnx_utils.EXPORT_TO_ONNX_DIRECT = True\n",
            "ir_graph_op_handler.KEEP_ORIGINAL_MODEL_STRUCTURE = False\n",
            "\n",
            "from aimet_utils.rmsnorm_update import RmsNorm, RmsNormOphandler\n",
            "# Update ir graph op handler's registry with new RmsNormOpHandler class\n",
            "from aimet_torch.pro import ir_graph_op_handler\n",
            "ir_graph_op_handler.ir_to_handler_dict['RmsNorm'] = RmsNormOphandler\n",
            "\n",
            "# Register RmsNorm op definition in custom_modules_for_qnn\n",
            "from aimet_torch.pro import custom_modules_for_qnn\n",
            "setattr(custom_modules_for_qnn, 'RmsNorm', RmsNorm)\n",
            "\n",
            "\n",
            "\n",
            "dummy_input, all_layer_kv_cache = get_dummy_data()\n",
            "input_names = list(dummy_input.keys())\n",
            "output_names = ['logits'] \n",
            "for i in range(config.num_hidden_layers):\n",
            "    output_names.append(f'past_key_{i}_out')\n",
            "    output_names.append(f'past_value_{i}_out')\n",
            "\n",
            "# Build converter args\n",
            "converter_args_param = ['--input_layout']\n",
            "converter_args_value = 'NONTRIVIAL'\n",
            "converter_args = []\n",
            "for input_param in converter_args_param:\n",
            "    for input_name in input_names:\n",
            "        converter_args += [input_param, input_name, converter_args_value]\n",
            "\n",
            "skip_prepare = False # This is done only once\n",
            "prepare_path = os.path.join(output_dir, 'prepare')\n",
            "os.makedirs(prepare_path, exist_ok=True)\n",
            "prepare_filename = f'{model_name}_kvcache_{config.num_hidden_layers}_layer'\n",
            "\n",
            "\n",
            "if skip_prepare:\n",
            "    with event_marker(f\"KVCache load pre-prepared {prepare_filename}\", flush_ram=True):\n",
            "        prepared_model_path = os.path.join(prepare_path, f'{prepare_filename}.py')\n",
            "        if not os.path.exists(prepared_model_path):\n",
            "            raise ValueError(f\"prepared artifacts not found in {prepare_path}\")\n",
            "        else:\n",
            "            print(f'WARNING: preparation skipped for model={prepare_filename}, prepared at {time.ctime(os.path.getmtime(prepared_model_path))}')\n",
            "            prepared_model = load_pytorch_model(path=prepare_path, filename=prepare_filename,\n",
            "                                                model_name=prepare_filename, load_state_dict=True)\n",
            "\n",
            "else:\n",
            "    with event_marker(\"KVCache prepare model\", flush_ram=True):\n",
            "        if(enable_fp16):\n",
            "            convert_model_to_fp32(model)\n",
            "        dummy_input_for_prepare = {\n",
            "            \"input_ids\": dummy_inputs[\"input_ids\"],\n",
            "            \"cos\": dummy_inputs[\"position_ids_cos\"],\n",
            "            \"sin\": dummy_inputs[\"position_ids_sin\"],\n",
            "            \"attention_mask\": dummy_inputs[\"attention_mask\"],\n",
            "            \"all_layers_kv_cache\": all_layer_kv_cache\n",
            "        }\n",
            "        prepared_model = model_preparer.prepare_model(model,\n",
            "                                                      dummy_input_for_prepare,\n",
            "                                                      model_name=prepare_filename,\n",
            "                                                      filename=prepare_filename,\n",
            "                                                      path=prepare_path,\n",
            "                                                      input_names=input_names,\n",
            "                                                      output_names=output_names,\n",
            "                                                      onnx_export_args={\"opset_version\":14},\n",
            "                                                      converter_args=converter_args)\n",
            "del model # original model no longer needed"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 7.2 Convert prepared model to FP16\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [],
         "source": [
            "if(enable_fp16):\n",
            "   convert_model_to_fp16(prepared_model)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 7.3 Model prepare verification\n",
            "\n",
            "Verify if prepared KV cache model generates the same PPL as FP model.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "metadata": {},
         "outputs": [],
         "source": [
            "prepared_model.to(device)\n",
            "\n",
            "def ppl_eval(test_dataset, model, num_batches=0, data_type=torch.float32):\n",
            "    first_sample = next(iter(test_dataset))[\"input_ids\"]\n",
            "    first_input_ids = torch.tensor(first_sample, dtype=torch.long, device=device).unsqueeze(0)\n",
            "    curr_len = first_input_ids.shape[1]\n",
            "    _, attention_mask, cos, sin, all_layer_kv_cache = qnn_llm_utils.prepare_inputs(first_input_ids)\n",
            "\n",
            "    attention_mask = attention_mask.to(data_type)\n",
            "    cos = cos.to(data_type)\n",
            "    sin = sin.to(data_type)\n",
            "    all_layer_kv_cache = [kv_cache.to(data_type) for kv_cache in all_layer_kv_cache]\n",
            "    \n",
            "    loss = 0\n",
            "    if num_batches == 0:\n",
            "        num_batches = num_total_batches\n",
            "    else:\n",
            "        num_batches = min(num_batches, num_total_batches)\n",
            "    for batch_id, batch in enumerate(tqdm(test_dataset, total=num_batches, desc=\"Evaluating PPL\")):\n",
            "        if batch_id >= num_batches:\n",
            "            break\n",
            "\n",
            "        input_ids = batch[\"input_ids\"]\n",
            "        input_ids = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
            "        curr_len = input_ids.shape[1]\n",
            "\n",
            "        input_ids = torch.cat(\n",
            "            [input_ids, torch.full((1, SEQ_LEN - curr_len), qnn_llm_utils.eos_token_id, device=input_ids.device)],\n",
            "            dim=-1,\n",
            "        )\n",
            "\n",
            "        with torch.no_grad():\n",
            "            all_inputs = (input_ids, cos, sin, attention_mask, *all_layer_kv_cache)\n",
            "            output = model(*all_inputs)\n",
            "\n",
            "        lm_logits = output[0]\n",
            "        shift_logits = lm_logits[:, :-1, :][:, :curr_len, :].contiguous().to(dtype=torch.float32)\n",
            "        shift_labels = input_ids[:, 1:][:, :curr_len].contiguous().to(shift_logits.device)\n",
            "\n",
            "        loss_fct = CrossEntropyLoss()\n",
            "        loss += loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
            "\n",
            "    loss = loss / num_batches\n",
            "    ppl = loss.exp()\n",
            "    return ppl"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:30:26,875 - Utils - INFO - memory usage @ 'KVcache prepared FP eval[gc] >> ' : GPU default:3.8 GB, RAM 3.3 GB\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Evaluating PPL: 100%|██████████| 20/20 [00:00<00:00, 37.00it/s]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:30:34,973 - Utils - INFO - memory usage @ 'KVcache prepared FP eval[gc] << ' : GPU default:9.1 GB, RAM 3.3 GB\n",
                  "2025-07-26 16:30:34,973 - Utils - INFO - Event KVcache prepared FP eval[gc] : time=0:00:08, GPU=5.4 GB(+ 3.8 GB), RAM=1.1 MB(+ 3.3 GB)\n",
                  "ppl score of KVCACHE prepared fp model: 2703445.5\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "\n"
               ]
            }
         ],
         "source": [
            "with event_marker(\"KVcache prepared FP eval\", flush_ram=True):\n",
            "    with torch.no_grad():\n",
            "        prepared_kvcache_ppl = ppl_eval(test_dataset, prepared_model, num_batches=20)\n",
            "print(f\"ppl score of KVCACHE prepared fp model: {prepared_kvcache_ppl}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---\n",
            "\n",
            "## 8. Quantization\n",
            "\n",
            "The _Quantization_ step is the primary focus of this notebook, this section could be modified to execute various quantization experiments.\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---\n",
            "\n",
            "### 8.1 Create quantsim configured for QNN HTP target\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:30:42,582 - Utils - INFO - memory usage @ 'create KVCache Quantsim >> ' : GPU default:5.0 GB, RAM 3.3 GB\n",
                  "2025-07-26 16:30:42,765 - Quant - INFO - Unsupported op type BatchPermutation\n",
                  "2025-07-26 16:30:42,765 - Quant - INFO - Unsupported op type CropAndResize\n",
                  "2025-07-26 16:30:42,766 - Quant - INFO - Unsupported op type BatchToSpace\n",
                  "2025-07-26 16:30:42,766 - Quant - INFO - Unsupported op type SpaceToBatch\n",
                  "2025-07-26 16:30:42,766 - Quant - INFO - Unsupported op type GroupNormalization\n",
                  "2025-07-26 16:30:42,767 - Quant - INFO - Unsupported op type LayerNormalization\n",
                  "2025-07-26 16:30:42,767 - Quant - INFO - Unsupported op type Mean\n",
                  "2025-07-26 16:30:42,768 - Quant - INFO - Unsupported op type RMSNormalization\n",
                  "2025-07-26 16:30:42,768 - Quant - INFO - Unsupported op type Squeeze\n",
                  "2025-07-26 16:30:42,768 - Quant - INFO - Unsupported op type Unsqueeze\n",
                  "2025-07-26 16:30:42,769 - Quant - INFO - Unsupported op type Compress\n",
                  "2025-07-26 16:30:42,769 - Quant - INFO - Unsupported op type Identity\n",
                  "2025-07-26 16:30:42,770 - Quant - INFO - Unsupported op type Shape\n",
                  "2025-07-26 16:30:42,771 - Quant - INFO - Unsupported op type If\n",
                  "2025-07-26 16:30:42,774 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:V73\n",
                  "2025-07-26 16:30:42,799 - Utils - INFO - memory usage @ 'create KVCache Quantsim << ' : GPU default:8.0 GB, RAM 3.3 GB\n",
                  "2025-07-26 16:30:42,800 - Utils - INFO - Event create KVCache Quantsim : time=0:00:00, GPU=3.0 GB(+ 5.0 GB), RAM=1.1 MB(+ 3.3 GB)\n"
               ]
            }
         ],
         "source": [
            "from aimet_common.defs import QuantScheme\n",
            "from aimet_torch.v2.quantsim import QuantizationSimModel\n",
            "import copy\n",
            "\n",
            "sim_model = copy.deepcopy(prepared_model)\n",
            "sim_model.to(device)\n",
            "\n",
            "def get_dummy_data():\n",
            "    input_ids = next(iter(test_dataset))[\"input_ids\"]\n",
            "    input_ids = torch.tensor(input_ids, dtype=torch.long, device=\"cuda\").unsqueeze(0)\n",
            "    input_ids, attention_mask, cos, sin, all_layer_kv_cache = qnn_llm_utils.prepare_inputs(input_ids)\n",
            "    inputs_values = [input_ids, cos, sin, attention_mask]\n",
            "    inputs_values.extend(all_layer_kv_cache)\n",
            "    inputs_keys = ['input_ids', 'position_ids_cos', 'position_ids_sin','attention_mask'] \n",
            "    \n",
            "    kv_inputs_keys = []\n",
            "    for i in range(config.num_hidden_layers):\n",
            "        kv_inputs_keys.append(f\"past_key_{i}_in\")\n",
            "        kv_inputs_keys.append(f\"past_value_{i}_in\")\n",
            "    inputs_keys.extend(kv_inputs_keys)\n",
            "    inputs = dict(zip(inputs_keys, inputs_values))\n",
            "    return inputs, all_layer_kv_cache\n",
            "\n",
            "dummy_input, _ = get_dummy_data()\n",
            "dummy_input_for_quantsim = tuple(dummy_input.values())\n",
            "with event_marker(\"create KVCache Quantsim\"):\n",
            "    quantsim = QuantizationSimModel(model=sim_model,\n",
            "                                    quant_scheme=QuantScheme.post_training_tf,\n",
            "                                    dummy_input=dummy_input_for_quantsim,\n",
            "                                    default_output_bw=16,\n",
            "                                    default_param_bw=4,\n",
            "                                    in_place=True,\n",
            "                                    config_file=htp_config_file)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---\n",
            "\n",
            "### 8.2 Setting 16bit x 8bit matmuls\n",
            "\n",
            "To keep key and value tensors as 8 bits, reducing data I/O costs associated with KV-cache orchestration.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [],
         "source": [
            "from aimet_torch.v2.experimental.quantsim_utils import set_matmul_second_input_producer_to_8bit_symmetric\n",
            "\n",
            "set_matmul_second_input_producer_to_8bit_symmetric(quantsim)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---\n",
            "\n",
            "### 8.3 Concat encoding unification\n",
            "\n",
            "configuring concat ops to have shared encoding on input and output activations.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "metadata": {},
         "outputs": [],
         "source": [
            "from aimet_torch.v2.experimental import propagate_output_encodings\n",
            "import aimet_torch.elementwise_ops as aimet_ops\n",
            "propagate_output_encodings(quantsim, aimet_ops.Concat)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---\n",
            "\n",
            "### 8.4 Manual Mixed Precision\n",
            "\n",
            "applying mixed precision configuration to ops\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Applying \\w*lm_head_(MatMul|conv_Conv):\t{'param_exceptions': {'bitwidth': 8}}\n",
                  "Applying \\w*norm_(Mul_1|Mul_1.module):\t{'input_exceptions': [{'input_index': 0, 'bitwidth': 16, 'asymmetric': True}]}\n",
                  "Applying \\w*norm_(Pow|Pow.module|ReduceMean|Add|Sqrt|Div|Mul):\t{'output_exceptions': [{'output_index': 0, 'enabled': False}]}\n",
                  "Applying \\w*v_proj_(MatMul|conv_Conv):\t{'output_exceptions': [{'output_index': 0, 'bitwidth': 8, 'asymmetric': False}]}\n",
                  "Applying \\w*Concat_\\d+:\t{'output_exceptions': [{'output_index': 0, 'bitwidth': 8, 'asymmetric': False}]}\n",
                  "Applying QuantizedRmsNorm:\t{'param_exceptions': {'asymmetric': True, 'bitwidth': 16}}\n"
               ]
            }
         ],
         "source": [
            "from llm_utils.mixed_precision_overrides import ManualQuantsimMixedPrecisionConfig\n",
            "quantsim_adjuster = ManualQuantsimMixedPrecisionConfig(mixed_precision_config_file= \"./config/mixed_precision_config/exceptions.json\")\n",
            "quantsim_adjuster.apply_exceptions(quantsim)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---\n",
            "\n",
            "### 8.5 Sequential MSE\n",
            "\n",
            "applying sequential MSE technique to optimize parameter encodings\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "metadata": {},
         "outputs": [],
         "source": [
            "from aimet_torch.v2.seq_mse import apply_seq_mse\n",
            "from aimet_torch.seq_mse import SeqMseParams\n",
            "from aimet_torch.utils import load_pytorch_model\n",
            "from torch.utils.data import DataLoader\n",
            "\n",
            "train_dataset = load_dataset(\"nexa4ai/qwen3_wiki_calib\", split=\"train\")\n",
            "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
            "\n",
            "\n",
            "input_ids = next(iter(train_dataloader))[\"input_ids\"]\n",
            "input_ids = torch.tensor(input_ids, dtype=torch.long, device=\"cuda\").unsqueeze(0)\n",
            "input_ids, attention_mask, cos, sin, all_layer_kv_cache = qnn_llm_utils.prepare_inputs(input_ids)\n",
            "\n",
            "def _forward_fn(model, inputs):\n",
            "    # slice inputs so that we only end up doing inference using first n tokens\n",
            "    input_ids = inputs[\"input_ids\"]\n",
            "    input_ids = torch.tensor(input_ids, dtype=torch.long, device=\"cuda\").unsqueeze(0)\n",
            "    curr_len = input_ids.shape[1]\n",
            "    input_ids = torch.cat(\n",
            "        [input_ids, torch.full((1, 2073 - curr_len), 151645, device=input_ids.device)],\n",
            "        dim=-1,\n",
            "    )\n",
            "    all_inputs = (input_ids, cos, sin, attention_mask, *all_layer_kv_cache)\n",
            "    model(*all_inputs)\n",
            "\n",
            "\n",
            "## HACK: We change num_batches from 20 to 1 to save time during the learning stage.\n",
            "params = SeqMseParams(num_batches=20,\n",
            "                      inp_symmetry=\"symqt\",\n",
            "                      num_candidates=20,\n",
            "                      loss_fn=\"mse\",\n",
            "                      forward_fn=_forward_fn)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 20,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:30:52,146 - Utils - INFO - memory usage @ 'SeqMSE >> ' : GPU default:5.0 GB, RAM 3.3 GB\n",
                  "2025-07-26 16:30:53,863 - Utils - INFO - Caching 20 batches from data loader at path location: /tmp/tmp5eizl03u/cached_dataset\n",
                  "2025-07-26 16:30:53,922 - SeqMse - INFO - Finding and freezing optimal param encodings candidate of module: q_proj_conv_Conv\n",
                  "2025-07-26 16:30:59,485 - SeqMse - INFO - Finding and freezing optimal param encodings candidate of module: k_proj_conv_Conv\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/home/azureuser/.conda/envs/aimet/lib/python3.10/site-packages/aimet_torch/seq_mse.py:461: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
                  "  cand_max = torch.tensor(per_channel_max / num_candidates * (cand + 1))\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:31:03,067 - SeqMse - INFO - Finding and freezing optimal param encodings candidate of module: v_proj_conv_Conv\n",
                  "2025-07-26 16:31:06,683 - SeqMse - INFO - Finding and freezing optimal param encodings candidate of module: o_proj_conv_Conv\n",
                  "2025-07-26 16:31:10,244 - SeqMse - INFO - Finding and freezing optimal param encodings candidate of module: gate_proj_conv_Conv\n",
                  "2025-07-26 16:31:14,010 - SeqMse - INFO - Finding and freezing optimal param encodings candidate of module: up_proj_conv_Conv\n",
                  "2025-07-26 16:31:17,785 - SeqMse - INFO - Finding and freezing optimal param encodings candidate of module: down_proj_conv_Conv\n",
                  "2025-07-26 16:31:21,667 - Utils - INFO - memory usage @ 'SeqMSE << ' : GPU default:8.0 GB, RAM 3.3 GB\n",
                  "2025-07-26 16:31:21,668 - Utils - INFO - Event SeqMSE : time=0:00:29, GPU=3.0 GB(+ 5.0 GB), RAM=6.6 MB(+ 3.3 GB)\n"
               ]
            }
         ],
         "source": [
            "with event_marker(\"SeqMSE\"):\n",
            "    prepared_model.to(\"cuda\")\n",
            "    quantsim.model.to(\"cuda\")\n",
            "    apply_seq_mse(prepared_model, quantsim, train_dataloader, params)\n",
            "\n",
            "del prepared_model"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "scrolled": true
         },
         "source": [
            "---\n",
            "\n",
            "### 8.6 Calibration\n",
            "\n",
            "compute activation encodings using AIMET\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 21,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:31:31,194 - Utils - INFO - memory usage @ 'compute encoding[gc] >> ' : GPU default:3.8 GB, RAM 3.3 GB\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "100%|██████████| 20/20 [00:01<00:00, 18.31it/s]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:31:32,312 - Utils - INFO - memory usage @ 'compute encoding[gc] << ' : GPU default:7.4 GB, RAM 3.3 GB\n",
                  "2025-07-26 16:31:32,312 - Utils - INFO - Event compute encoding[gc] : time=0:00:01, GPU=3.5 GB(+ 3.8 GB), RAM=1.3 MB(+ 3.3 GB)\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "\n"
               ]
            }
         ],
         "source": [
            "from torch.utils.data import DataLoader\n",
            "train_dataset = load_dataset(\"nexa4ai/qwen3_wiki_calib\", split=\"train\")\n",
            "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
            "\n",
            "\n",
            "input_ids = next(iter(train_dataloader))[\"input_ids\"]\n",
            "input_ids = torch.tensor(input_ids, dtype=torch.long, device=\"cuda\").unsqueeze(0)\n",
            "input_ids, attention_mask, cos, sin, all_layer_kv_cache = qnn_llm_utils.prepare_inputs(input_ids)\n",
            "\n",
            "def _forward_fn(model, kwargs):\n",
            "    data_loader = kwargs['data_loader']\n",
            "    max_iterations = kwargs['num_batches']\n",
            "    \n",
            "    for batch_id, batch in enumerate(tqdm(data_loader, total=max_iterations)):\n",
            "        if batch_id < max_iterations:\n",
            "            input_ids = batch['input_ids']\n",
            "            input_ids = torch.tensor(input_ids, dtype=torch.long, device=\"cuda\").unsqueeze(0)\n",
            "            curr_len = input_ids.shape[1]\n",
            "            input_ids = torch.cat(\n",
            "                [input_ids, torch.full((1, 2073 - curr_len), 151645, device=input_ids.device)],\n",
            "                dim=-1,\n",
            "            )\n",
            "            all_inputs = (input_ids, cos, sin, attention_mask, *all_layer_kv_cache)\n",
            "            model(*all_inputs)\n",
            "        else:\n",
            "            break\n",
            "        \n",
            "kwargs = {\n",
            "   'data_loader': train_dataloader,\n",
            "   'num_batches': 20\n",
            "}\n",
            "\n",
            "with event_marker(\"compute encoding\", flush_ram=True):\n",
            "    quantsim.model.to(\"cuda\")\n",
            "    quantsim.compute_encodings(_forward_fn, kwargs)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 8.7 Eval KV Cache sim model.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 22,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:31:32,493 - Utils - INFO - memory usage @ 'KV cache sim eval[gc] >> ' : GPU default:3.8 GB, RAM 3.3 GB\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Evaluating PPL: 100%|██████████| 20/20 [00:01<00:00, 12.82it/s]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:31:41,594 - Utils - INFO - memory usage @ 'KV cache sim eval[gc] << ' : GPU default:9.7 GB, RAM 3.3 GB\n",
                  "2025-07-26 16:31:41,595 - Utils - INFO - Event KV cache sim eval[gc] : time=0:00:09, GPU=5.9 GB(+ 3.8 GB), RAM=32.5 MB(+ 3.3 GB)\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "ppl score of KVCACHE sim fp model: 2184613.75\n"
               ]
            }
         ],
         "source": [
            "with event_marker(\"KV cache sim eval\", flush_ram=True):\n",
            "    with torch.no_grad():\n",
            "        quantsim.model.to(\"cuda\")\n",
            "        sim_ppl = ppl_eval(test_dataset, quantsim.model, num_batches=20)\n",
            "\n",
            "print(f\"ppl score of KVCACHE sim fp model: {sim_ppl}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 8.8 Real Test\n",
            "\n",
            "We conduct the real LLM inference process to ensure that the quantized model can do the reasonable inference.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 23,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[node[nodeeleeleeleeleelelementlementlement\n"
               ]
            }
         ],
         "source": [
            "eos_token_id = 151645\n",
            "eos_tokens = {151645}\n",
            "input_text = (\n",
            "    \"<|im_start|>user\\nIntroduce Newton's first law of motion. Be short and concise.<|im_end|>\"\n",
            "    \"\\n<|im_start|>assistant\\n\"\n",
            ")\n",
            "input_ids_list = tokenizer.encode(input_text)  # a list of ints\n",
            "\n",
            "n_past = 0\n",
            "curr_len = len(input_ids_list)\n",
            "\n",
            "if curr_len < SEQ_LEN:\n",
            "    input_ids_list = input_ids_list + [eos_token_id] * (SEQ_LEN - curr_len)\n",
            "\n",
            "input_ids = torch.tensor(input_ids_list, dtype=torch.long, device=device).unsqueeze(0)\n",
            "attention_mask = qnn_llm_utils.get_attention_mask(n_past, curr_len)\n",
            "position_ids = qnn_llm_utils.get_position_ids(n_past, SEQ_LEN)\n",
            "cos, sin = qnn_llm_utils.get_cos_sin(attention_mask, position_ids)  # attn_mask as dummpy input to get device\n",
            "all_layer_kv_caches = qnn_llm_utils.get_kv_cache()\n",
            "last_token_indices = torch.tensor([n_past + curr_len - 1], dtype=torch.long, device=device)\n",
            "\n",
            "generated_ids = []\n",
            "\n",
            "for i in range(10):\n",
            "    with torch.no_grad():\n",
            "        prepared_inputs = (input_ids, cos, sin, attention_mask, *all_layer_kv_caches)\n",
            "        outputs = quantsim.model(*prepared_inputs)\n",
            "        next_token_logits = outputs[0][:, last_token_indices, :]\n",
            "        next_token_id = torch.argmax(next_token_logits, dim=-1).item()\n",
            "        generated_ids.append(next_token_id)\n",
            "\n",
            "        ## update the inputs for next token\n",
            "        all_layer_kv_caches = qnn_llm_utils.update_kv_cache(all_layer_kv_caches, outputs[1:], n_past, curr_len)\n",
            "        n_past += curr_len\n",
            "        curr_len = 1\n",
            "        next_input_ids = [next_token_id] + [eos_token_id] * (SEQ_LEN - 1)\n",
            "        input_ids = torch.tensor(next_input_ids, dtype=torch.long, device=device).unsqueeze(\n",
            "            0\n",
            "        )  # set bs_size = 1 manually\n",
            "        last_token_indices = torch.tensor([curr_len - 1], dtype=torch.long, device=device)\n",
            "        attention_mask = qnn_llm_utils.get_attention_mask(n_past, curr_len)\n",
            "        position_ids = qnn_llm_utils.get_position_ids(n_past, SEQ_LEN)\n",
            "        cos, sin = qnn_llm_utils.get_cos_sin(attention_mask, position_ids)\n",
            "\n",
            "    if next_token_id in eos_tokens:\n",
            "        break\n",
            "\n",
            "generated_text = tokenizer.decode(generated_ids)\n",
            "print(generated_text)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---\n",
            "\n",
            "## 9. Export\n",
            "\n",
            "the pipeline call below would export onnx model, encoding and test vector for KVCache models.\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---\n",
            "\n",
            "### 9.1 Generating test vectors for QNN SDK\n",
            "\n",
            "We actually only use one piece of data. Thus, the computation can be done in CPU, since this step can be easily OOM on CUDA.\n",
            "\n",
            "<span style=\"color:#d62728;font-weight:bold;\">TODO</span>: We still need to figure out what this is, and how we are going to use it. We didn't run through this cell for now.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 24,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:31:42,548 - Utils - INFO - memory usage @ 'generate test vector >> ' : GPU default:3.8 GB, RAM 3.3 GB\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Test vector generation:   0%|          | 0/1 [00:00<?, ?it/s]/home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-1/llm_utils/test_vectors.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
                  "  return nested_map(t, lambda x: torch.tensor(x) if isinstance(x, QuantizedTensorBase) else x)\n",
                  "Test vector generation: 100%|██████████| 1/1 [01:10<00:00, 70.92s/it]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:32:53,707 - Utils - INFO - memory usage @ 'generate test vector << ' : GPU default:3.8 GB, RAM 22.7 GB\n",
                  "2025-07-26 16:32:53,708 - Utils - INFO - Event generate test vector : time=0:01:11, GPU=0.0 bytes(+ 3.8 GB), RAM=19.4 GB(+ 3.3 GB)\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "\n"
               ]
            }
         ],
         "source": [
            "%load_ext autoreload\n",
            "%autoreload 2\n",
            "from llm_utils.test_vectors import generate_test_vectors\n",
            "# del prepared_model  \n",
            "test_vector_layers = [\n",
            "    \"model_layers_\\\\d+_input_layernorm_Pow\",\n",
            "    \"lm_head_conv_Conv\"\n",
            "]\n",
            "with event_marker(\"generate test vector\"):\n",
            "    quantsim.model.to(\"cpu\")\n",
            "    generate_test_vectors(quantsim, qnn_llm_utils, train_dataloader, output_dir, \n",
            "                          num_batches=1, test_vector_layers=test_vector_layers, input_names=input_names)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---\n",
            "\n",
            "### 9.2 Export KVCache Model\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 25,
         "metadata": {},
         "outputs": [],
         "source": [
            "dummy_input, _ = get_dummy_data()\n",
            "dummy_input = tuple(dummy_input.values())"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 26,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:33:01,582 - Utils - INFO - memory usage @ 'KVCache export[gc] >> ' : GPU default:2.7 GB, RAM 5.4 GB\n",
                  "2025-07-26 16:33:01,584 - Quant - WARNING - Exporting encodings to yaml will be deprecated in a future release. Ensure that your code can work with the exported files ending in \".encodings\" which are saved using json format. For the time being, if yaml export is needed, set aimet_common.utils.SAVE_TO_YAML to True.\n",
                  "2025-07-26 16:33:12,482 - Quant - WARNING - number of input quantizers: 1 available for layer: Reshape doesn't match with number of input tensors: 2\n",
                  "2025-07-26 16:33:12,525 - Quant - WARNING - number of input quantizers: 1 available for layer: Slice doesn't match with number of input tensors: 5\n",
                  "2025-07-26 16:33:12,527 - Quant - WARNING - number of input quantizers: 1 available for layer: Slice_1 doesn't match with number of input tensors: 5\n",
                  "2025-07-26 16:33:12,533 - Quant - WARNING - number of input quantizers: 1 available for layer: Slice_2 doesn't match with number of input tensors: 5\n",
                  "2025-07-26 16:33:12,535 - Quant - WARNING - number of input quantizers: 1 available for layer: Slice_3 doesn't match with number of input tensors: 5\n",
                  "2025-07-26 16:33:12,560 - Quant - WARNING - number of input quantizers: 1 available for layer: Reshape_10 doesn't match with number of input tensors: 2\n",
                  "2025-07-26 16:33:12,631 - Quant - WARNING - number of input quantizers: 1 available for layer: Reshape_12 doesn't match with number of input tensors: 2\n",
                  "2025-07-26 16:33:15,299 - Quant - INFO - Layers excluded from quantization: []\n",
                  "2025-07-26 16:33:19,150 - Utils - INFO - memory usage @ 'KVCache export[gc] << ' : GPU default:2.7 GB, RAM 11.6 GB\n",
                  "2025-07-26 16:33:19,151 - Utils - INFO - Event KVCache export[gc] : time=0:00:17, GPU=1.3 MB(+ 2.7 GB), RAM=6.2 GB(+ 5.4 GB)\n"
               ]
            }
         ],
         "source": [
            "from aimet_torch.utils import change_tensor_device_placement\n",
            "from aimet_torch.onnx_utils import OnnxExportApiArgs\n",
            "onnx_dir = os.path.join(output_dir, 'onnx')\n",
            "os.makedirs(onnx_dir, exist_ok=True)\n",
            "\n",
            "if(enable_fp16):\n",
            "    # Convert FP16 model back to FP32 for ONNX export\n",
            "    convert_model_to_fp32(quantsim.model)\n",
            "\n",
            "quantsim.model.to(\"cpu\")\n",
            "onnx_api_args = OnnxExportApiArgs(input_names=input_names,output_names=output_names, opset_version=14)\n",
            "sample_inputs = change_tensor_device_placement(dummy_input, torch.device('cpu'))\n",
            "with event_marker(\"KVCache export\", flush_ram=True):\n",
            "    quantsim.export(onnx_dir, model_name, sample_inputs, onnx_export_args=onnx_api_args)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---\n",
            "\n",
            "### Summary\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 27,
         "metadata": {
            "jp-MarkdownHeadingCollapsed": true
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-26 16:33:19,196 - Utils - INFO - #0: Event FP eval : time=0:00:07, GPU=5.4 GB(+ 2.5 GB), RAM=33.0 MB(+ 3.1 GB)\n",
                  "2025-07-26 16:33:19,197 - Utils - INFO - #1: Event KVCache prepare model[gc] : time=0:00:50, GPU=5.1 GB(+ 2.6 GB), RAM=4.9 GB(+ 3.2 GB)\n",
                  "2025-07-26 16:33:19,198 - Utils - INFO - #2: Event KVcache prepared FP eval[gc] : time=0:00:08, GPU=5.4 GB(+ 3.8 GB), RAM=1.1 MB(+ 3.3 GB)\n",
                  "2025-07-26 16:33:19,198 - Utils - INFO - #3: Event create KVCache Quantsim : time=0:00:00, GPU=3.0 GB(+ 5.0 GB), RAM=1.1 MB(+ 3.3 GB)\n",
                  "2025-07-26 16:33:19,199 - Utils - INFO - #4: Event SeqMSE : time=0:00:29, GPU=3.0 GB(+ 5.0 GB), RAM=6.6 MB(+ 3.3 GB)\n",
                  "2025-07-26 16:33:19,199 - Utils - INFO - #5: Event compute encoding[gc] : time=0:00:01, GPU=3.5 GB(+ 3.8 GB), RAM=1.3 MB(+ 3.3 GB)\n",
                  "2025-07-26 16:33:19,200 - Utils - INFO - #6: Event KV cache sim eval[gc] : time=0:00:09, GPU=5.9 GB(+ 3.8 GB), RAM=32.5 MB(+ 3.3 GB)\n",
                  "2025-07-26 16:33:19,200 - Utils - INFO - #7: Event generate test vector : time=0:01:11, GPU=0.0 bytes(+ 3.8 GB), RAM=19.4 GB(+ 3.3 GB)\n",
                  "2025-07-26 16:33:19,200 - Utils - INFO - #8: Event KVCache export[gc] : time=0:00:17, GPU=1.3 MB(+ 2.7 GB), RAM=6.2 GB(+ 5.4 GB)\n",
                  "2025-07-26 16:33:19,201 - Utils - INFO - Max GPU memory used: 9.7 GB bytes for event(s) KV cache sim eval[gc]\n",
                  "2025-07-26 16:33:19,201 - Utils - INFO - Max CPU memory used: 22.7 GB bytes for event(s) generate test vector\n"
               ]
            }
         ],
         "source": [
            "from aimet_torch.pro.utils.profiler import EventProfiler\n",
            "EventProfiler().report()\n",
            "EventProfiler().json_dump(os.path.join(output_dir, 'profiling_stats'))\n",
            "\n",
            "import json\n",
            "with open(f'{output_dir}/ppl.json', 'wt') as f:\n",
            "    json.dump({\n",
            "        \"original\": float(orig_ppl),\n",
            "        \"prepared_kvcache\": float(prepared_kvcache_ppl),\n",
            "        \"QuantSim\": float(sim_ppl),\n",
            "    }, f, indent=2)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Copyright (c) 2024 Qualcomm Technologies, Inc. and/or its subsidiaries.\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "aimet",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.16"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}
