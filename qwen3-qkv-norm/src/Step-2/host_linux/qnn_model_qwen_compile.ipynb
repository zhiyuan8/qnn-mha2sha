{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5549491",
   "metadata": {},
   "source": [
    "# QNN Model Prepare on Linux\n",
    "\n",
    "The Qualcomm AI Engine Direct SDK allows clients to run ML models on HTP hardware. The following steps describe how to prepare the TinyLlama 1.1B model on Linux platforms for Android platform with HTP capability.\n",
    "\n",
    "Before continuing, ensure all steps from [README](README.md) are completed. \n",
    "\n",
    "This document uses the term Qualcomm Neural Network (QNN) and Qualcomm AI Engine Direct SDK interchangeably.\n",
    "\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "1. Qualcomm AI Engine Direct SDK (with Ubuntu Linux support)\n",
    "2. Ubuntu 22.04 installation with required packages for QNN Tools\n",
    "3. Android Platform tools version 31 or greater\n",
    "4. This notebook could be executed with Anaconda (with the supplied environment.yaml) or a virtual environment (venv)\n",
    "5. TinyLlama `.onnx` files and their corresponding AIMET encodings (generated via AIMET workflow)\n",
    "\n",
    "This work flow assumes that you have generated the artifacts following the AIMET TinyLlama workflow:\n",
    "\n",
    "- TinyLlama 1.1B model and its AIMET encodings\n",
    "- `.pkl` file per network - a numpy object array saved as a Python pickle that contains data that is required as part of the model conversion step. \n",
    "\n",
    "![dir_struct](../assets/step-1_output_dir_contents.png \"Overall directory Structure from notebook 1\")\n",
    "\n",
    "# Workflow\n",
    "\n",
    "\n",
    "All the models and encodings are processed independently via different executable QNN utilities available in the Qualcomm AI Engine Direct SDK.\n",
    "\n",
    "To prepare TinyLlama 1.1B models for inference, the QNN executable utilities require an Ubuntu 22.04 environment\n",
    "\n",
    "1. Generate the AR-1 and AR-128 onnx models from the AR-1073 exported model.\n",
    "2. Model splitting is not required to run TinyLlama 1.1B (w4a16) on NSP, so set num_splits=1.\n",
    "3. Apply MHA2SHA transformation to convert all attention block MHAs to SHAs.\n",
    "4. Convert the `.onnx` files to their equivalent QNN representation.\n",
    "5. Generate the QNN model quantized libraries.\n",
    "6. Generate the QNN context binaries for the QNN HTP backend.\n",
    "\n",
    "After preparing the TinyLlama 1.1B model for inference, the next step is to execute the QNN context binaries for inference on a Snapdragon Android device.\n",
    "\n",
    "\n",
    "![QNN Work flow](../assets/qnn-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76588239-7e79-4830-91a6-ce02eb5147e7",
   "metadata": {},
   "source": [
    "### Setup AIMET model export directory\n",
    "1. Create a folder called `assets` inside example2/host_linux.\n",
    "\n",
    "2. Create a folder called `models` inside `assets` (example2/host_linux/assets).\n",
    "\n",
    "3. The `assets/models` path must contain following AIMET model export artifacts:\n",
    "\n",
    "       `onnx` folder: Containing .onnx and .encodings files\n",
    "\n",
    "       `test_vectors` folder: Containing .pkl files for QNN conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b089024",
   "metadata": {},
   "source": [
    "### Configure QNN SDK path\n",
    "\n",
    "The following step configures the Qualcomm AI Engine Direct SDK, which enables running TinyLlama 1.1B on the device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b19dea-37df-471f-8a2b-f2663e3fd4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ln -sf /tmp/qnn assets/qnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a8d95",
   "metadata": {},
   "source": [
    "### Install the required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f70fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a6fd1a",
   "metadata": {},
   "source": [
    "## Set up models and Qualcomm AI Engine Direct SDK variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7bb6c8",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "import time\n",
    "from pathlib import Path\n",
    "# setup whether using multithread or single thread to compile\n",
    "go_parallel = True\n",
    "\n",
    "workfolder = os.getcwd()\n",
    "\n",
    "# Set up environment variable to reference LLAMA_MODELS\n",
    "LLAMA_MODELS = workfolder + \"/assets/models\"\n",
    "\n",
    "# Set QNN_SDK_ROOT environment variable to the location of Qualcomm AI Engine Directory\n",
    "QNN_SDK_ROOT = '/opt/qcom/aistack/qairt/2.34.2.250528/'\n",
    "\n",
    "# Check path to LLAMA_MODELS and QNN_SDK_ROOT \n",
    "assert os.path.exists(QNN_SDK_ROOT) == True,\"QNN_SDK_ROOT path does not exist\"\n",
    "assert os.path.exists(LLAMA_MODELS) == True,\"LLAMA_MODELS path does not exist\"\n",
    "os.environ['QNN_SDK_ROOT'] = QNN_SDK_ROOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e32d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All task list: ['ar1-1', 'ar128-1']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(workfolder+'/../../../common/G2G')\n",
    "sys.path.append(workfolder+'/../../../common/G2G/split_onnx_utils')\n",
    "sys.path.append(workfolder+'/../../../common/')\n",
    "from utilities.nsptargets import NspTargets\n",
    "from utilities.profiler import event_marker\n",
    "\n",
    "# Set up nsp target specification\n",
    "nsp_target = NspTargets.Windows.GEN2\n",
    "\n",
    "CL = 4096\n",
    "ARNs = [1,128]\n",
    "EXPORT_AR = 2073\n",
    "EXPORT_CONTEXT_LENGTH = 4096\n",
    "onnx_name = \"qwen3\"\n",
    "# Model splitting is not required to run TinyLlama (w4a16) on NSP, so set num_splits=1\n",
    "num_splits = 1\n",
    "\n",
    "splits = range(1, num_splits+1)\n",
    "arn_list = [ arn for arn in ARNs for i in splits ]\n",
    "split_idxs = [i for arn in ARNs for i in splits]\n",
    "print('All task list:', [f\"ar{arn}-{n}\" for arn,n in zip(arn_list,split_idxs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ef8c1",
   "metadata": {},
   "source": [
    "# Prepare TinyLlama 1.1B model for Inference\n",
    "\n",
    "The following section uses the Qualcomm AI Engine Direct SDK to prepare TinyLlama 1.1B model for on-target inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9676413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.conda/envs/aimet/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking graph input/output/value_infoChecking graph input/output/value_info\n",
      "\n",
      "[1, 2073] => [1, 128] : input_ids\n",
      "[1, 2073] => [1, 1] : input_ids[1, 1, 2073, 64] => [1, 1, 128, 64] : position_ids_cos\n",
      "\n",
      "[1, 1, 2073, 64] => [1, 1, 1, 64] : position_ids_cos[1, 1, 2073, 64] => [1, 1, 128, 64] : position_ids_sin\n",
      "\n",
      "[1, 1, 2073, 4096] => [1, 1, 128, 4096] : attention_mask[1, 1, 2073, 64] => [1, 1, 1, 64] : position_ids_sin\n",
      "\n",
      "[1, 8, 128, 2023] => [1, 8, 128, 3968] : past_key_0_in[1, 1, 2073, 4096] => [1, 1, 1, 4096] : attention_mask\n",
      "\n",
      "[1, 8, 2023, 128] => [1, 8, 3968, 128] : past_value_0_in[1, 8, 128, 2023] => [1, 8, 128, 4095] : past_key_0_in\n",
      "\n",
      "[1, 2073, 151936] => [1, 128, 151936] : logits[1, 8, 2023, 128] => [1, 8, 4095, 128] : past_value_0_in\n",
      "\n",
      "[1, 8, 128, 2073] => [1, 8, 128, 128] : past_key_0_out[1, 2073, 151936] => [1, 1, 151936] : logits\n",
      "\n",
      "[1, 8, 2073, 128] => [1, 8, 128, 128] : past_value_0_out[1, 8, 128, 2073] => [1, 8, 128, 1] : past_key_0_out\n",
      "\n",
      "Checking initializer[1, 8, 2073, 128] => [1, 8, 1, 128] : past_value_0_out\n",
      "\n",
      "Checking initializer\n",
      "Checking node attributesChecking node attributes\n",
      "\n",
      "[-1, 2073, 1, 1024] => [-1, 128, 1, 1024] : [-1, 2073, 1, 1024] => [-1, 1, 1, 1024] : \n",
      "\n",
      "[-1, 16, 128, 2073] => [-1, 16, 128, 128] : [-1, 16, 128, 2073] => [-1, 16, 128, 1] : \n",
      "\n",
      "[-1, 8, 128, 2073] => [-1, 8, 128, 128] : [-1, 8, 128, 2073] => [-1, 8, 128, 1] : \n",
      "\n",
      "[-1, 8, 128, 2073] => [-1, 8, 128, 128] : [-1, 8, 128, 2073] => [-1, 8, 128, 1] : \n",
      "\n",
      "[-1, 2073, 1, 2048] => [-1, 128, 1, 2048] : [-1, 2073, 1, 2048] => [-1, 1, 1, 2048] : \n",
      "\n",
      "[-1, 2073, 1024] => [-1, 128, 1024] : [-1, 2073, 1024] => [-1, 1, 1024] : \n",
      "\n",
      "[-1, 2073, 1, 1024] => [-1, 128, 1, 1024] : [-1, 2073, 1, 1024] => [-1, 1, 1, 1024] : \n",
      "\n",
      "[-1, 2073, 1024] => [-1, 128, 1024] : [-1, 2073, 1024] => [-1, 1, 1024] : \n",
      "\n",
      "[-1, 2073, 1, 1024] => [-1, 128, 1, 1024] : [-1, 2073, 1, 1024] => [-1, 1, 1, 1024] : \n",
      "\n",
      "[-1, 2073, 151936] => [-1, 128, 151936] : [-1, 2073, 151936] => [-1, 1, 151936] : \n",
      "\n",
      "Done. Fixed 19 occurrencesDone. Fixed 19 occurrences\n",
      "\n",
      "Saving as /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/models_ar_n/ar128-cl4096/onnx/qwen3.onnx, with 19 changesSaving as /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/models_ar_n/ar1-cl4096/onnx/qwen3.onnx, with 19 changes\n",
      "\n",
      "Loading /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/models/test_vectors/qt_0.pklLoading /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/models/test_vectors/qt_0.pkl\n",
      "\n",
      "Resize [1, 2073] => [1, 1]\n",
      "Resize [1, 1, 2073, 64] => [1, 1, 1, 64]\n",
      "Resize [1, 1, 2073, 64] => [1, 1, 1, 64]\n",
      "Resize [1, 1, 2073, 4096] => [1, 1, 1, 4096]\n",
      "Resize [1, 2073] => [1, 128]\n",
      "Resize [1, 1, 2073, 64] => [1, 1, 128, 64]\n",
      "Resize [1, 1024, 1, 2073] => [1, 1024, 1, 1]Resize [1, 1, 2073, 64] => [1, 1, 128, 64]\n",
      "\n",
      "Resize [1, 1, 2073, 4096] => [1, 1, 128, 4096]\n",
      "Resize [1, 1024, 1, 2073] => [1, 1024, 1, 128]Resize [1, 151936, 1, 2073] => [1, 151936, 1, 1]\n",
      "\n",
      "Resize [1, 151936, 1, 2073] => [1, 151936, 1, 128]\n",
      "Resize [1, 151936, 1, 2073] => [1, 151936, 1, 1]\n",
      "Resize [1, 151936, 1, 2073] => [1, 151936, 1, 128]\n",
      "Resize [1, 2073, 1, 151936] => [1, 1, 1, 151936]\n",
      "Resize [1, 2073, 1, 151936] => [1, 128, 1, 151936]\n",
      "Resize [1, 8, 128, 2073] => [1, 8, 128, 1]\n",
      "Resize [1, 8, 2073, 128] => [1, 8, 1, 128]\n",
      "Resize [1, 2073, 151936] => [1, 1, 151936]\n",
      "Resize [1, 8, 128, 2073] => [1, 8, 128, 128]\n",
      "Resize [1, 8, 2073, 128] => [1, 8, 128, 128]\n",
      "Resize [1, 2073, 151936] => [1, 128, 151936]\n",
      "Resize [1, 8, 128, 2023] => [1, 8, 128, 3968]\n",
      "Resize [1, 8, 2023, 128] => [1, 8, 3968, 128]Resize [1, 8, 128, 2023] => [1, 8, 128, 4095]\n",
      "\n",
      "Resize [1, 8, 2023, 128] => [1, 8, 4095, 128]\n",
      "Saving /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/models_ar_n/ar128-cl4096/test_vectors/qt_0.pklSaving /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/models_ar_n/ar1-cl4096/test_vectors/qt_0.pkl\n",
      "\n",
      "Loading /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/models/test_vectors/fp_0.pkl\n",
      "Loading /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/models/test_vectors/fp_0.pkl\n",
      "Resize [1, 2073] => [1, 1]\n",
      "Resize [1, 1, 2073, 64] => [1, 1, 1, 64]\n",
      "Resize [1, 1, 2073, 64] => [1, 1, 1, 64]\n",
      "Resize [1, 1, 2073, 4096] => [1, 1, 1, 4096]\n",
      "Resize [1, 1024, 1, 2073] => [1, 1024, 1, 1]\n",
      "Resize [1, 151936, 1, 2073] => [1, 151936, 1, 1]\n",
      "Resize [1, 151936, 1, 2073] => [1, 151936, 1, 1]\n",
      "Resize [1, 2073, 1, 151936] => [1, 1, 1, 151936]\n",
      "Resize [1, 2073] => [1, 128]\n",
      "Resize [1, 1, 2073, 64] => [1, 1, 128, 64]\n",
      "Resize [1, 1, 2073, 64] => [1, 1, 128, 64]\n",
      "Resize [1, 1, 2073, 4096] => [1, 1, 128, 4096]\n",
      "Resize [1, 1024, 1, 2073] => [1, 1024, 1, 128]\n",
      "Resize [1, 151936, 1, 2073] => [1, 151936, 1, 128]\n",
      "Resize [1, 151936, 1, 2073] => [1, 151936, 1, 128]\n",
      "Resize [1, 2073, 1, 151936] => [1, 128, 1, 151936]\n",
      "Resize [1, 8, 128, 2073] => [1, 8, 128, 1]\n",
      "Resize [1, 8, 128, 2073] => [1, 8, 128, 128]\n",
      "Prepare AR128 AR1 export done.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(f\"{workfolder}/assets/models_ar_n\", exist_ok=True)\n",
    "\n",
    "import change_hardcoding\n",
    "def gen_ar(arn):\n",
    "    change_hardcoding.execute(\n",
    "            f\"{LLAMA_MODELS}\", \n",
    "            f\"{workfolder}/assets/models_ar_n/ar{arn}-cl{CL}\", \n",
    "            [f\" {EXPORT_AR},{arn}\",f\" -{EXPORT_AR},-1\",f\" {EXPORT_CONTEXT_LENGTH},{CL}\",f\" {EXPORT_CONTEXT_LENGTH-EXPORT_AR},{CL-arn}\"]\n",
    "            )\n",
    "\n",
    "with event_marker(f'prepare-export'):\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers = len(ARNs) if go_parallel else 1) as executor:\n",
    "        results = executor.map(gen_ar,ARNs)\n",
    "print(f\"Prepare AR128 AR1 export done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8b0b5",
   "metadata": {},
   "source": [
    "## Preprocess ONNX \n",
    "\n",
    "Prior to utilizing the QNN tool chain to compile and generate the context binary for LLM we may need to split the model and generate the following artifacts\n",
    "- ONNX file for each split of the model\n",
    "- input vectors for each split\n",
    "- golden output vectors for each split\n",
    "\n",
    "TinyLlama 1.1B (w4a16) does not require model splitting to run on NSP targets, but the notebook supports the model splitting logic if required for a use-case.\n",
    "\n",
    "To run TinyLlama 1.1B as a full model without splitting, we set **num_splits=1** above in the notebook.\n",
    "\n",
    "We need to specify the following parameters to proceed with execution of the notebook and generate all necessary artifacts\n",
    "- number of splits of the model (set to 1 to compile TinyLlama as a full model without splitting)\n",
    "- path to TinyLlama onnx file\n",
    "- path to TinyLlama encodings file\n",
    "- path to *.pkl files \n",
    "  \n",
    "\n",
    "![Split](../assets/ModelSplit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ca4ae",
   "metadata": {},
   "source": [
    "### Set up environment variables for the Qualcomm AI Direct SDK tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "317c4f67",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "\n",
    "qnn_env = os.environ.copy()\n",
    "qnn_env[\"QNN_SDK_ROOT\"] = QNN_SDK_ROOT\n",
    "qnn_env[\"PYTHONPATH\"] = QNN_SDK_ROOT + \"/benchmarks/QNN/:\" + QNN_SDK_ROOT + \"/lib/python\"\n",
    "qnn_env[\"PATH\"] = QNN_SDK_ROOT + \"/bin/x86_64-linux-clang:\" + qnn_env[\"PATH\"]\n",
    "qnn_env[\"LD_LIBRARY_PATH\"] = QNN_SDK_ROOT + \"/lib/x86_64-linux-clang\"\n",
    "qnn_env[\"HEXAGON_TOOLS_DIR\"] = QNN_SDK_ROOT + \"/bin/x86_64-linux-clang\"\n",
    "qnn_env[\"LLM\"] = \"1\"\n",
    "qnn_env[\"split_embedding\"] = \"0\"\n",
    "qnn_env[\"split_lmhead\"] = \"0\"\n",
    "os.environ = qnn_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df707d",
   "metadata": {},
   "source": [
    "### Split Onnx export\n",
    "\n",
    "This step splits a model into multiple parts based on the number of splits specified.\n",
    "\n",
    "Expected execution time: ~< 15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de3e7406",
   "metadata": {
    "lines_to_next_cell": 1,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting qwen3.onnxStarting qwen3.onnx\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/artifacts/ar1-cl4096//src/onnx/qwen3.onnxLoading /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/artifacts/ar128-cl4096//src/onnx/qwen3.onnx\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per_layer_output_names: Per_layer_output_names:['/Add_4/Add_output_0'] \n",
      "['/Add_4/Add_output_0']\n",
      "Using per-layer output shape: [1, 128, 1024]\n",
      "Using per-layer output shape: [1, 1, 1024]\n",
      "Names_to_split Names_to_split[] \n",
      "[]\n",
      "Saving /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/artifacts/ar128-cl4096//split_onnx/ar128-cl4096_1_of_1.onnx\n",
      "Saving /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/artifacts/ar1-cl4096//split_onnx/ar1-cl4096_1_of_1.onnx\n",
      "/home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/models_ar_n/ar128-cl4096/test_vectors/qt\n",
      "/home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/models_ar_n/ar1-cl4096/test_vectors/qt\n",
      "Mapping test vector 'lm_head_conv_Conv' to '/lm_head_conv_Conv/Conv'\n",
      "Mapping test vector 'lm_head_conv_Conv_output_0_nchw' to '/lm_head_conv_Conv_output_0_nchw/Transpose'\n",
      "Ending qwen3.onnx\n",
      "Mapping test vector 'lm_head_conv_Conv' to '/lm_head_conv_Conv/Conv'\n",
      "Mapping test vector 'lm_head_conv_Conv_output_0_nchw' to '/lm_head_conv_Conv_output_0_nchw/Transpose'\n",
      "Ending qwen3.onnx\n",
      "All onnx model splitted.\n"
     ]
    }
   ],
   "source": [
    "def thread_split(arn):\n",
    "    name = f\"ar{arn}-cl{CL}\"\n",
    "    model_export = f\"{workfolder}/assets/models_ar_n\"\n",
    "    model_artifact = f\"{workfolder}/assets/artifacts/ar{arn}-cl{CL}/\"\n",
    "    os.makedirs(model_artifact, exist_ok = True)\n",
    "    \n",
    "    # create symlink to export\n",
    "    symlink_src = os.path.join(model_artifact, 'src')\n",
    "    symlink_path = Path(symlink_src)\n",
    "    if symlink_path.is_symlink():\n",
    "        os.unlink(symlink_src)\n",
    "    os.symlink(src = os.path.join(model_export, name), dst = symlink_src)\n",
    "    \n",
    "    os.makedirs(f\"{model_artifact}/split_onnx\", exist_ok = True)\n",
    "    TEST_VECTOR_PICKLE_TYPE = \"pkl\"\n",
    "    print(f\"Starting {onnx_name}.onnx\")\n",
    "    utils.split_onnx(onnxfile = f\"{model_artifact}/src/onnx/{onnx_name}.onnx\", modelname = name, \n",
    "                     pickle_filedir = os.path.join(model_export, f\"ar{arn}-cl{CL}/test_vectors\"),\n",
    "                     num_splits = num_splits, output_dir = model_artifact, split_embedding = False,\n",
    "                     encoding_file = f\"{model_artifact}/src/onnx/{onnx_name}.encodings\",using_qairt_workflow = True\n",
    "                     )\n",
    "    print(f\"Ending {onnx_name}.onnx\")\n",
    "\n",
    "with event_marker(f'split-onnx'):\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers = len(ARNs) if go_parallel else 1) as executor:\n",
    "        results = executor.map(thread_split,ARNs)\n",
    "print(f\"All onnx model splitted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb7eb16",
   "metadata": {},
   "source": [
    "### Convert attention layers from MHA to SHA\n",
    "\n",
    "The `mha2sha-onnx-converter` tool converts a model from MHA representation to its equivalent SHA representation. The encoding files generated from the AIMET workflow are provided as an input to this step via the `--exported-model-encoding-path` option.\n",
    "\n",
    "This step generates a new `.onnx` file that represents the model in SHA format.\n",
    "\n",
    "Expected execution time: ~10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d0e3343",
   "metadata": {
    "lines_to_next_cell": 1,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHA2SHA tool root set to: /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/../../../common/G2G/MHA2SHA\n",
      "mha2sha-onnx-converter ar1-cl4096_1_of_1 running...\n",
      "\u001b[3m                             \u001b[0m\u001b[1;3;94m Qualcomm MHA2SHA\u001b[0m\u001b[3m                              \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mFlag                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue\u001b[0m\u001b[1m                                  \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│ --ar-num [ To be deprecate]    │ \u001b[1mNone\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --base-llm                     │ \u001b[1m\"llama3\"\u001b[0m                                │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --build-ar                     │ \u001b[1mNone\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --create-input-lists           │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --disable-auto-attn-finder     │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --exported-model-encoding-path │ \u001b[1m\"qwen3.encodings\"\u001b[0m                       │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --gqa-model                    │ \u001b[1;32mTrue\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --handle-alibi                 │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --handle-internal-rmsnorm      │ ⚠  \u001b[1;6;31mTrue\u001b[0m                                 │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --handle-past-key-value        │ \u001b[1;32mTrue\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --handle-r3-matrix             │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --handle-rope-ops              │ \u001b[1;32mTrue\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --llm-model                    │ \u001b[1;32mTrue\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --log-level                    │ \u001b[1m\"debug\"\u001b[0m                                 │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --lora-adaptor-list-path       │ \u001b[1mNone\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --lora-alpha-from-input        │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --lora-model                   │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --mha-conv                     │ ⚠  \u001b[1;6;31mTrue\u001b[0m                                 │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --mha-lora-tensor-path         │ \u001b[1mNone\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --nchw-aligned                 │ ⚠  \u001b[1;6;31mTrue\u001b[0m                                 │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --no-verification              │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --not-strict                   │ \u001b[1;32mTrue\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --optimize-o-proj              │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --position-ids                 │ \u001b[1mNone\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --prepared-model               │ \u001b[1;32mTrue\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --replace-linear-with-conv     │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --skip-mha2sha                 │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --strict                       │ \u001b[1;32mTrue\u001b[0m                                    │\n",
      "└────────────────────────────────┴─────────────────────────────────────────┘\n",
      "\u001b[2;3m                              \u001b[0m\u001b[1;2;3m Version: \u001b[0m\u001b[1;2;3;94m1.2.0\u001b[0m\u001b[2;3m                               \u001b[0m\n",
      "\u001b[2;3m                      \u001b[0m\u001b[1;2;3m Model byte size: \u001b[0m\u001b[1;2;3;94m1,307,604,507\u001b[0m\u001b[2;3m                       \u001b[0m\n",
      "\n",
      "\n",
      "\u001b[3m              ⚠  --base-llm 'llama3' Mapped Args Overwritten ⚠              \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mFlag                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m'llama3' Default\u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mUser Provided   \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ --handle-internal-rmsnorm │ \u001b[1;31mFalse\u001b[0m                     │ \u001b[1;32mTrue\u001b[0m             │\n",
      "├───────────────────────────┼───────────────────────────┼──────────────────┤\n",
      "│ --mha-conv                │ \u001b[1;31mFalse\u001b[0m                     │ \u001b[1;32mTrue\u001b[0m             │\n",
      "├───────────────────────────┼───────────────────────────┼──────────────────┤\n",
      "│ --nchw-aligned            │ \u001b[1;31mFalse\u001b[0m                     │ \u001b[1;32mTrue\u001b[0m             │\n",
      "└───────────────────────────┴───────────────────────────┴──────────────────┘\n",
      "╭──────────────────────────────────────────────────────────────────────────╮\n",
      "│                                                                          │\n",
      "│     The \u001b[3m--base-llm\u001b[0m flag is used to turn on necessary flags for specific  │\n",
      "│ model                                                                    │\n",
      "│     architectures. For example, \u001b[3m--base-llm llama2\u001b[0m would implictly turn   │\n",
      "│ on flags                                                                 │\n",
      "│     such as \u001b[3m--handle-rope-ops\u001b[0m. However, some of these flags were         │\n",
      "│ overwritten                                                              │\n",
      "│     by user provided arguments.                                          │\n",
      "│                                                                          │\n",
      "╰──────────────────────────────────────────────────────────────────────────╯\n",
      " 2025-07-26 17:13:24,922 - 78 - INFO - Step 1: Loading model\n",
      "\n",
      "2025-07-26 17:13:24,922 - 78 - INFO - Loading the onnx model from: /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/artifacts/ar1-cl4096//split_onnx/ar1-cl4096_1_of_1.onnx\n",
      "2025-07-26 17:13:25,337 - 60 - DEBUG - '--create-input-lists' overridden to 'False'\n",
      "2025-07-26 17:13:25,338 - 60 - DEBUG - '--disable-auto-attn-finder' overridden to 'False'\n",
      "2025-07-26 17:13:25,338 - 60 - DEBUG - '--gqa-model' overridden to 'True'\n",
      "2025-07-26 17:13:25,338 - 60 - DEBUG - '--handle-past-key-value' overridden to 'True'\n",
      "2025-07-26 17:13:25,338 - 60 - DEBUG - '--handle-rope-ops' overridden to 'True'\n",
      "2025-07-26 17:13:25,338 - 60 - DEBUG - '--llm-model' overridden to 'True'\n",
      "2025-07-26 17:13:25,338 - 83 - WARNING - Flag '--mha-conv' is set to 'True' for 'llama3' but got 'True' explicitly. Setting flag to explict value.\n",
      "2025-07-26 17:13:25,338 - 83 - WARNING - Flag '--nchw-aligned' is set to 'False' for 'llama3' but got 'True' explicitly. Setting flag to explict value.\n",
      "2025-07-26 17:13:25,338 - 60 - DEBUG - '--no-verification' overridden to 'False'\n",
      "2025-07-26 17:13:25,338 - 60 - DEBUG - '--prepared-model' overridden to 'True'\n",
      "2025-07-26 17:13:25,338 - 60 - DEBUG - '--strict' overridden to 'True'\n",
      "2025-07-26 17:13:25,349 - 78 - INFO - --------------------\n",
      "2025-07-26 17:13:25,349 - 78 - INFO - Step 2: Checking the correctness of `model` object\n",
      "\n",
      "2025-07-26 17:13:25,706 - 78 - INFO - --------------------\n",
      "2025-07-26 17:13:25,706 - 78 - INFO - Step 3: Generating model inputs and outputs\n",
      "\n",
      "2025-07-26 17:13:27,691 - 78 - INFO - --------------------\n",
      "2025-07-26 17:13:27,692 - 78 - INFO - Step 4: Running auto pattern matcher on model object.\n",
      "\n",
      "2025-07-26 17:13:27,693 - 78 - INFO - Running pattern matcher - pattern matched:\n",
      "2025-07-26 17:13:27,693 - 78 - INFO - MatMul Div Add Softmax MatMul\n",
      "2025-07-26 17:13:27,693 - 78 - INFO - found_matched_pattern: 1\n",
      "2025-07-26 17:13:27,693 - 78 - INFO - --------------------\n",
      "2025-07-26 17:13:27,693 - 78 - INFO - Step 5: Applying MHA2SHA optimization on model object\n",
      "\n",
      "2025-07-26 17:13:28,139 - 78 - INFO - Step 5.1: Apply prequant model adaption.\n",
      "\n",
      "2025-07-26 17:13:28,143 - 78 - INFO - Step 5.2: Apply mha2sha model adaption.\n",
      "\n",
      "2025-07-26 17:13:28,144 - 60 - DEBUG - No AR value provided, empty initing ArBuilder - NOT buildable\n",
      "2025-07-26 17:13:28,144 - 78 - INFO - \n",
      "\n",
      "Start node: 0, /MatMul/MatMul -> /MatMul_1/MatMul\n",
      "2025-07-26 17:13:28,144 - 60 - DEBUG - QKV MatMuls : /q_proj_conv_Conv/Conv, /k_proj_conv_Conv/Conv, /v_proj_conv_Conv/Conv\n",
      "2025-07-26 17:13:28,146 - 60 - DEBUG - Node: '/Reshape_1/Reshape' met end search criteria.\n",
      "2025-07-26 17:13:28,149 - 60 - DEBUG - Node: '/Reshape_2/Reshape' met end search criteria.\n",
      "2025-07-26 17:13:28,149 - 78 - INFO - head_dim: 128 num_heads: 16\n",
      "2025-07-26 17:13:28,571 - 78 - INFO - Saving lora weights to safetensor...\n",
      "2025-07-26 17:13:28,571 - 78 - INFO - --------------------\n",
      "2025-07-26 17:13:28,571 - 78 - INFO - Step 5.3: Create sha encodings.\n",
      "\n",
      "2025-07-26 17:13:28,571 - 78 - INFO - Found AIMET encoding version: 0.6.1\n",
      "2025-07-26 17:13:29,439 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: qk_matmul key: /Reshape_6/Reshape_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,439 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: qk_scale key: Constant_32_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,439 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: qkv_matmul key: /Reshape_7/Reshape_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: pow_ key: /rms_norm_2/Constant_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: pow_ key: /rms_norm_2/Pow_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: reduce_mean key: /rms_norm_2/ReduceMean_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: add key: /rms_norm_2/Constant_1_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: add key: /rms_norm_2/Add_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: sqrt key: /rms_norm_2/Sqrt_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: div key: /rms_norm_2/Sqrt_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: div key: /rms_norm_2/Div_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: out_mul key: /rms_norm_2/Div_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: pow_ key: /rms_norm_4/Constant_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: pow_ key: /rms_norm_4/Pow_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: reduce_mean key: /rms_norm_4/ReduceMean_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: add key: /rms_norm_4/Constant_1_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: add key: /rms_norm_4/Add_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: sqrt key: /rms_norm_4/Sqrt_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: div key: /rms_norm_4/Sqrt_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: div key: /rms_norm_4/Div_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,440 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: out_mul key: /rms_norm_4/Div_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,441 - 83 - WARNING - Got PTQ param encoding on rms_norm_2.weight, applying same param encoding to all these sha weights: ['rmsnormMul/value1_8', 'rmsnormMul/value1_9', 'rmsnormMul/value1_10', 'rmsnormMul/value1_11', 'rmsnormMul/value1_12', 'rmsnormMul/value1_13', 'rmsnormMul/value1_14', 'rmsnormMul/value1_15', 'rmsnormMul/value1_16', 'rmsnormMul/value1_17', 'rmsnormMul/value1_18', 'rmsnormMul/value1_19', 'rmsnormMul/value1_20', 'rmsnormMul/value1_21', 'rmsnormMul/value1_22', 'rmsnormMul/value1_23'].\n",
      "2025-07-26 17:13:29,441 - 83 - WARNING - Got PTQ param encoding on rms_norm_4.weight, applying same param encoding to all these sha weights: ['rmsnormMul/value1', 'rmsnormMul/value1_1', 'rmsnormMul/value1_2', 'rmsnormMul/value1_3', 'rmsnormMul/value1_4', 'rmsnormMul/value1_5', 'rmsnormMul/value1_6', 'rmsnormMul/value1_7'].\n",
      "2025-07-26 17:13:29,441 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: Mul_rope_cos_1 key: /Slice/Slice_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,441 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: Mul_rope_sin_1 key: /Slice/Slice_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,441 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: Mul_rope_cos_2 key: /Slice_1/Slice_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,441 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: Mul_rope_sin_2 key: /Slice_1/Slice_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,441 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: Mul_rope_cos_1 key: /Slice_2/Slice_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,441 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: Mul_rope_sin_1 key: /Slice_2/Slice_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,441 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: Mul_rope_cos_2 key: /Slice_3/Slice_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,441 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: Mul_rope_sin_2 key: /Slice_3/Slice_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:29,444 - 60 - DEBUG - activation encoding: /Expand/Mul_output_0 is not input or output to any node in SHA model. Removing from sha_encoding.\n",
      "2025-07-26 17:13:29,444 - 60 - DEBUG - activation encoding: /Expand_1/Mul_output_0 is not input or output to any node in SHA model. Removing from sha_encoding.\n",
      "2025-07-26 17:13:29,444 - 60 - DEBUG - activation encoding: Expand_1_coef is not input or output to any node in SHA model. Removing from sha_encoding.\n",
      "2025-07-26 17:13:29,444 - 60 - DEBUG - activation encoding: Expand_coef is not input or output to any node in SHA model. Removing from sha_encoding.\n",
      "2025-07-26 17:13:31,210 - 78 - INFO - SHA encodings saved at /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/artifacts/ar1-cl4096/1_of_1/sha_output/ar1-cl4096_1_of_1.encodings\n",
      "2025-07-26 17:13:31,211 - 78 - INFO - Step 5.4: Saving converted model...\n",
      "\n",
      "2025-07-26 17:13:31,212 - 78 - INFO - Saving model with `save_as_external_data`\n",
      "2025-07-26 17:13:33,255 - 78 - INFO - MHA2SHA model saved at: /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/artifacts/ar1-cl4096/1_of_1/sha_output/ar1-cl4096_1_of_1.onnx\n",
      "2025-07-26 17:13:33,285 - 78 - INFO - --------------------\n",
      "2025-07-26 17:13:33,286 - 78 - INFO - Step 6: Comparing MHA2SHA model to Original by running on ONNXRT\n",
      "\n",
      "2025-07-26 17:13:33,286 - 78 - INFO - Load and run SHA model from /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/artifacts/ar1-cl4096/1_of_1/sha_output/ar1-cl4096_1_of_1.onnx\n",
      "2025-07-26 17:13:33,286 - 78 - INFO - Running SHA model with ORT\n",
      "2025-07-26 17:13:33,286 - 60 - DEBUG - Permuting inputs for past key/value inputs\n",
      "2025-07-26 17:13:34,440 - 60 - DEBUG - output_names[i]='logits' _golden_outputs.shape=(1, 1, 151936) _converted_model_outputs.shape=(1, 1, 151936)\n",
      "2025-07-26 17:13:34,441 - 60 - DEBUG - For logits : MAD = 6.914139e-06\n",
      "\n",
      "2025-07-26 17:13:34,441 - 60 - DEBUG - output_names[i]='past_key_0_out' _golden_outputs.shape=(1, 8, 128, 1) _converted_model_outputs.shape=(8, 1, 128, 1)\n",
      "2025-07-26 17:13:34,442 - 60 - DEBUG - For past_key_0_out : MAD = 3.0517578e-05\n",
      "\n",
      "2025-07-26 17:13:34,442 - 60 - DEBUG - output_names[i]='past_value_0_out' _golden_outputs.shape=(1, 8, 1, 128) _converted_model_outputs.shape=(8, 1, 1, 128)\n",
      "2025-07-26 17:13:34,442 - 60 - DEBUG - For past_value_0_out : MAD = 0.0\n",
      "\n",
      "2025-07-26 17:13:34,442 - 78 - INFO - Total Runtime ----- 0:00:09 -----\n",
      "2025-07-26 17:13:34,442 - 78 - INFO - Verification Status ----- \u001b[92mOK\u001b[0m -----\n",
      "\n",
      "mha2sha-onnx-converter ar1-cl4096_1_of_1 done.\n",
      "mha2sha-onnx-converter ar128-cl4096_1_of_1 running...\n",
      "\u001b[3m                             \u001b[0m\u001b[1;3;94m Qualcomm MHA2SHA\u001b[0m\u001b[3m                              \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mFlag                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue\u001b[0m\u001b[1m                                  \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│ --ar-num [ To be deprecate]    │ \u001b[1mNone\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --base-llm                     │ \u001b[1m\"llama3\"\u001b[0m                                │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --build-ar                     │ \u001b[1mNone\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --create-input-lists           │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --disable-auto-attn-finder     │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --exported-model-encoding-path │ \u001b[1m\"qwen3.encodings\"\u001b[0m                       │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --gqa-model                    │ \u001b[1;32mTrue\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --handle-alibi                 │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --handle-internal-rmsnorm      │ ⚠  \u001b[1;6;31mTrue\u001b[0m                                 │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --handle-past-key-value        │ \u001b[1;32mTrue\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --handle-r3-matrix             │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --handle-rope-ops              │ \u001b[1;32mTrue\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --llm-model                    │ \u001b[1;32mTrue\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --log-level                    │ \u001b[1m\"debug\"\u001b[0m                                 │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --lora-adaptor-list-path       │ \u001b[1mNone\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --lora-alpha-from-input        │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --lora-model                   │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --mha-conv                     │ ⚠  \u001b[1;6;31mTrue\u001b[0m                                 │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --mha-lora-tensor-path         │ \u001b[1mNone\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --nchw-aligned                 │ ⚠  \u001b[1;6;31mTrue\u001b[0m                                 │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --no-verification              │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --not-strict                   │ \u001b[1;32mTrue\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --optimize-o-proj              │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --position-ids                 │ \u001b[1mNone\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --prepared-model               │ \u001b[1;32mTrue\u001b[0m                                    │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --replace-linear-with-conv     │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --skip-mha2sha                 │ \u001b[1;31mFalse\u001b[0m                                   │\n",
      "├────────────────────────────────┼─────────────────────────────────────────┤\n",
      "│ --strict                       │ \u001b[1;32mTrue\u001b[0m                                    │\n",
      "└────────────────────────────────┴─────────────────────────────────────────┘\n",
      "\u001b[2;3m                              \u001b[0m\u001b[1;2;3m Version: \u001b[0m\u001b[1;2;3;94m1.2.0\u001b[0m\u001b[2;3m                               \u001b[0m\n",
      "\u001b[2;3m                      \u001b[0m\u001b[1;2;3m Model byte size: \u001b[0m\u001b[1;2;3;94m1,307,604,515\u001b[0m\u001b[2;3m                       \u001b[0m\n",
      "\n",
      "\n",
      "\u001b[3m              ⚠  --base-llm 'llama3' Mapped Args Overwritten ⚠              \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mFlag                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m'llama3' Default\u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mUser Provided   \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ --handle-internal-rmsnorm │ \u001b[1;31mFalse\u001b[0m                     │ \u001b[1;32mTrue\u001b[0m             │\n",
      "├───────────────────────────┼───────────────────────────┼──────────────────┤\n",
      "│ --mha-conv                │ \u001b[1;31mFalse\u001b[0m                     │ \u001b[1;32mTrue\u001b[0m             │\n",
      "├───────────────────────────┼───────────────────────────┼──────────────────┤\n",
      "│ --nchw-aligned            │ \u001b[1;31mFalse\u001b[0m                     │ \u001b[1;32mTrue\u001b[0m             │\n",
      "└───────────────────────────┴───────────────────────────┴──────────────────┘\n",
      "╭──────────────────────────────────────────────────────────────────────────╮\n",
      "│                                                                          │\n",
      "│     The \u001b[3m--base-llm\u001b[0m flag is used to turn on necessary flags for specific  │\n",
      "│ model                                                                    │\n",
      "│     architectures. For example, \u001b[3m--base-llm llama2\u001b[0m would implictly turn   │\n",
      "│ on flags                                                                 │\n",
      "│     such as \u001b[3m--handle-rope-ops\u001b[0m. However, some of these flags were         │\n",
      "│ overwritten                                                              │\n",
      "│     by user provided arguments.                                          │\n",
      "│                                                                          │\n",
      "╰──────────────────────────────────────────────────────────────────────────╯\n",
      " 2025-07-26 17:13:34,807 - 78 - INFO - Step 1: Loading model\n",
      "\n",
      "2025-07-26 17:13:34,807 - 78 - INFO - Loading the onnx model from: /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/artifacts/ar128-cl4096//split_onnx/ar128-cl4096_1_of_1.onnx\n",
      "2025-07-26 17:13:35,222 - 60 - DEBUG - '--create-input-lists' overridden to 'False'\n",
      "2025-07-26 17:13:35,222 - 60 - DEBUG - '--disable-auto-attn-finder' overridden to 'False'\n",
      "2025-07-26 17:13:35,222 - 60 - DEBUG - '--gqa-model' overridden to 'True'\n",
      "2025-07-26 17:13:35,222 - 60 - DEBUG - '--handle-past-key-value' overridden to 'True'\n",
      "2025-07-26 17:13:35,222 - 60 - DEBUG - '--handle-rope-ops' overridden to 'True'\n",
      "2025-07-26 17:13:35,222 - 60 - DEBUG - '--llm-model' overridden to 'True'\n",
      "2025-07-26 17:13:35,222 - 83 - WARNING - Flag '--mha-conv' is set to 'True' for 'llama3' but got 'True' explicitly. Setting flag to explict value.\n",
      "2025-07-26 17:13:35,222 - 83 - WARNING - Flag '--nchw-aligned' is set to 'False' for 'llama3' but got 'True' explicitly. Setting flag to explict value.\n",
      "2025-07-26 17:13:35,222 - 60 - DEBUG - '--no-verification' overridden to 'False'\n",
      "2025-07-26 17:13:35,222 - 60 - DEBUG - '--prepared-model' overridden to 'True'\n",
      "2025-07-26 17:13:35,222 - 60 - DEBUG - '--strict' overridden to 'True'\n",
      "2025-07-26 17:13:35,234 - 78 - INFO - --------------------\n",
      "2025-07-26 17:13:35,234 - 78 - INFO - Step 2: Checking the correctness of `model` object\n",
      "\n",
      "2025-07-26 17:13:35,589 - 78 - INFO - --------------------\n",
      "2025-07-26 17:13:35,589 - 78 - INFO - Step 3: Generating model inputs and outputs\n",
      "\n",
      "2025-07-26 17:13:37,663 - 78 - INFO - --------------------\n",
      "2025-07-26 17:13:37,663 - 78 - INFO - Step 4: Running auto pattern matcher on model object.\n",
      "\n",
      "2025-07-26 17:13:37,664 - 78 - INFO - Running pattern matcher - pattern matched:\n",
      "2025-07-26 17:13:37,664 - 78 - INFO - MatMul Div Add Softmax MatMul\n",
      "2025-07-26 17:13:37,664 - 78 - INFO - found_matched_pattern: 1\n",
      "2025-07-26 17:13:37,664 - 78 - INFO - --------------------\n",
      "2025-07-26 17:13:37,664 - 78 - INFO - Step 5: Applying MHA2SHA optimization on model object\n",
      "\n",
      "2025-07-26 17:13:38,077 - 78 - INFO - Step 5.1: Apply prequant model adaption.\n",
      "\n",
      "2025-07-26 17:13:38,082 - 78 - INFO - Step 5.2: Apply mha2sha model adaption.\n",
      "\n",
      "2025-07-26 17:13:38,082 - 60 - DEBUG - No AR value provided, empty initing ArBuilder - NOT buildable\n",
      "2025-07-26 17:13:38,082 - 78 - INFO - \n",
      "\n",
      "Start node: 0, /MatMul/MatMul -> /MatMul_1/MatMul\n",
      "2025-07-26 17:13:38,082 - 60 - DEBUG - QKV MatMuls : /q_proj_conv_Conv/Conv, /k_proj_conv_Conv/Conv, /v_proj_conv_Conv/Conv\n",
      "2025-07-26 17:13:38,084 - 60 - DEBUG - Node: '/Reshape_1/Reshape' met end search criteria.\n",
      "2025-07-26 17:13:38,087 - 60 - DEBUG - Node: '/Reshape_2/Reshape' met end search criteria.\n",
      "2025-07-26 17:13:38,088 - 78 - INFO - head_dim: 128 num_heads: 16\n",
      "2025-07-26 17:13:38,488 - 78 - INFO - Saving lora weights to safetensor...\n",
      "2025-07-26 17:13:38,488 - 78 - INFO - --------------------\n",
      "2025-07-26 17:13:38,488 - 78 - INFO - Step 5.3: Create sha encodings.\n",
      "\n",
      "2025-07-26 17:13:38,488 - 78 - INFO - Found AIMET encoding version: 0.6.1\n",
      "2025-07-26 17:13:39,345 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: qk_matmul key: /Reshape_6/Reshape_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,345 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: qk_scale key: Constant_32_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,345 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: qkv_matmul key: /Reshape_7/Reshape_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: pow_ key: /rms_norm_2/Constant_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: pow_ key: /rms_norm_2/Pow_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: reduce_mean key: /rms_norm_2/ReduceMean_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: add key: /rms_norm_2/Constant_1_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: add key: /rms_norm_2/Add_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: sqrt key: /rms_norm_2/Sqrt_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: div key: /rms_norm_2/Sqrt_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: div key: /rms_norm_2/Div_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: out_mul key: /rms_norm_2/Div_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: pow_ key: /rms_norm_4/Constant_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: pow_ key: /rms_norm_4/Pow_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: reduce_mean key: /rms_norm_4/ReduceMean_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: add key: /rms_norm_4/Constant_1_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: add key: /rms_norm_4/Add_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: sqrt key: /rms_norm_4/Sqrt_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: div key: /rms_norm_4/Sqrt_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: div key: /rms_norm_4/Div_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: out_mul key: /rms_norm_4/Div_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,346 - 83 - WARNING - Got PTQ param encoding on rms_norm_2.weight, applying same param encoding to all these sha weights: ['rmsnormMul/value1_8', 'rmsnormMul/value1_9', 'rmsnormMul/value1_10', 'rmsnormMul/value1_11', 'rmsnormMul/value1_12', 'rmsnormMul/value1_13', 'rmsnormMul/value1_14', 'rmsnormMul/value1_15', 'rmsnormMul/value1_16', 'rmsnormMul/value1_17', 'rmsnormMul/value1_18', 'rmsnormMul/value1_19', 'rmsnormMul/value1_20', 'rmsnormMul/value1_21', 'rmsnormMul/value1_22', 'rmsnormMul/value1_23'].\n",
      "2025-07-26 17:13:39,347 - 83 - WARNING - Got PTQ param encoding on rms_norm_4.weight, applying same param encoding to all these sha weights: ['rmsnormMul/value1', 'rmsnormMul/value1_1', 'rmsnormMul/value1_2', 'rmsnormMul/value1_3', 'rmsnormMul/value1_4', 'rmsnormMul/value1_5', 'rmsnormMul/value1_6', 'rmsnormMul/value1_7'].\n",
      "2025-07-26 17:13:39,347 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: Mul_rope_cos_1 key: /Slice/Slice_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,347 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: Mul_rope_sin_1 key: /Slice/Slice_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,347 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: Mul_rope_cos_2 key: /Slice_1/Slice_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,347 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: Mul_rope_sin_2 key: /Slice_1/Slice_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,347 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: Mul_rope_cos_1 key: /Slice_2/Slice_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,347 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: Mul_rope_sin_1 key: /Slice_2/Slice_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,347 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: Mul_rope_cos_2 key: /Slice_3/Slice_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,347 - 60 - DEBUG - mha activation encoding for attention within /MatMul/MatMul, node: Mul_rope_sin_2 key: /Slice_3/Slice_output_0 not exist in mha encoding file\n",
      "2025-07-26 17:13:39,350 - 60 - DEBUG - activation encoding: /Expand/Mul_output_0 is not input or output to any node in SHA model. Removing from sha_encoding.\n",
      "2025-07-26 17:13:39,350 - 60 - DEBUG - activation encoding: /Expand_1/Mul_output_0 is not input or output to any node in SHA model. Removing from sha_encoding.\n",
      "2025-07-26 17:13:39,350 - 60 - DEBUG - activation encoding: Expand_1_coef is not input or output to any node in SHA model. Removing from sha_encoding.\n",
      "2025-07-26 17:13:39,351 - 60 - DEBUG - activation encoding: Expand_coef is not input or output to any node in SHA model. Removing from sha_encoding.\n",
      "2025-07-26 17:13:41,206 - 78 - INFO - SHA encodings saved at /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/artifacts/ar128-cl4096/1_of_1/sha_output/ar128-cl4096_1_of_1.encodings\n",
      "2025-07-26 17:13:41,206 - 78 - INFO - Step 5.4: Saving converted model...\n",
      "\n",
      "2025-07-26 17:13:41,208 - 78 - INFO - Saving model with `save_as_external_data`\n",
      "2025-07-26 17:13:43,262 - 78 - INFO - MHA2SHA model saved at: /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/artifacts/ar128-cl4096/1_of_1/sha_output/ar128-cl4096_1_of_1.onnx\n",
      "2025-07-26 17:13:43,289 - 78 - INFO - --------------------\n",
      "2025-07-26 17:13:43,290 - 78 - INFO - Step 6: Comparing MHA2SHA model to Original by running on ONNXRT\n",
      "\n",
      "2025-07-26 17:13:43,290 - 78 - INFO - Load and run SHA model from /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/artifacts/ar128-cl4096/1_of_1/sha_output/ar128-cl4096_1_of_1.onnx\n",
      "2025-07-26 17:13:43,290 - 78 - INFO - Running SHA model with ORT\n",
      "2025-07-26 17:13:43,290 - 60 - DEBUG - Permuting inputs for past key/value inputs\n",
      "2025-07-26 17:13:44,654 - 60 - DEBUG - output_names[i]='logits' _golden_outputs.shape=(1, 128, 151936) _converted_model_outputs.shape=(1, 128, 151936)\n",
      "2025-07-26 17:13:44,728 - 60 - DEBUG - For logits : MAD = 5.4478645e-05\n",
      "\n",
      "2025-07-26 17:13:44,728 - 60 - DEBUG - output_names[i]='past_key_0_out' _golden_outputs.shape=(1, 8, 128, 128) _converted_model_outputs.shape=(8, 1, 128, 128)\n",
      "2025-07-26 17:13:44,728 - 60 - DEBUG - For past_key_0_out : MAD = 0.0\n",
      "\n",
      "2025-07-26 17:13:44,728 - 60 - DEBUG - output_names[i]='past_value_0_out' _golden_outputs.shape=(1, 8, 128, 128) _converted_model_outputs.shape=(8, 1, 128, 128)\n",
      "2025-07-26 17:13:44,729 - 60 - DEBUG - For past_value_0_out : MAD = 0.0\n",
      "\n",
      "2025-07-26 17:13:44,729 - 78 - INFO - Total Runtime ----- 0:00:09 -----\n",
      "2025-07-26 17:13:44,729 - 78 - INFO - Verification Status ----- \u001b[92mOK\u001b[0m -----\n",
      "\n",
      "mha2sha-onnx-converter ar128-cl4096_1_of_1 done.\n",
      "All mha2sha convert done.\n"
     ]
    }
   ],
   "source": [
    "mha2sha_root = workfolder+\"/../../../common/G2G/MHA2SHA\"\n",
    "g2g_env = os.environ.copy()\n",
    "g2g_env[\"PYTHONPATH\"] = os.pathsep.join([g2g_env.get(\"PYTHONPATH\", \"\"), os.path.join(mha2sha_root, \"src/python\")])\n",
    "g2g_env[\"PATH\"] = os.pathsep.join([g2g_env.get(\"PATH\", \"\"), os.path.join(mha2sha_root, \"bin\")])\n",
    "print(f\"MHA2SHA tool root set to: {mha2sha_root}\")\n",
    "\n",
    "def thread_g2g(arn,split):\n",
    "    import os\n",
    "    os.chmod(os.path.join(mha2sha_root, \"bin\", \"mha2sha-onnx-converter\"), 0o777)\n",
    "    os.chmod(os.path.join(mha2sha_root, \"bin\", \"env_setup.sh\"), 0o777)\n",
    "    model_artifact = f\"{workfolder}/assets/artifacts/ar{arn}-cl{CL}/\"\n",
    "    split_work_dir = os.path.join(model_artifact,f\"{split}_of_{num_splits}\")\n",
    "    name = f\"ar{arn}-cl{CL}_{split}_of_{num_splits}\"\n",
    "    os.makedirs(split_work_dir, exist_ok = True)\n",
    "    sha_folder = f\"{split_work_dir}/sha_output/\"\n",
    "    os.makedirs(sha_folder, exist_ok = True)\n",
    "    name = f\"ar{arn}-cl{CL}_{split}_of_{num_splits}\"\n",
    "    print(f\"mha2sha-onnx-converter {name} running...\")\n",
    "    args=[\"mha2sha-onnx-converter\",\n",
    "                        \"--sha-export-path\", sha_folder,\n",
    "                        \"--model-name\", name,\n",
    "                        \"--exported-model-encoding-path\", f\"{model_artifact}/src/onnx/{onnx_name}.encodings\",\n",
    "                        \"--exported-model-path\", f\"{model_artifact}/split_onnx/{name}.onnx\",\n",
    "                        \"--base-llm\", \"llama3\",\n",
    "                        \"--mha-conv\",\n",
    "                        \"--nchw-aligned\",\n",
    "                        \"--handle-internal-rmsnorm\",\n",
    "                        \"--log-level\", \"debug\"]\n",
    "    proc = subprocess.Popen(args,stdout = subprocess.PIPE, stderr = subprocess.PIPE, env = g2g_env)\n",
    "    output, error = proc.communicate()\n",
    "    print(output.decode(),error.decode())\n",
    "    print(f\"mha2sha-onnx-converter {name} done.\")\n",
    "\n",
    "for arn, split in zip(arn_list, split_idxs):\n",
    "    thread_g2g(arn, split)\n",
    "print(f\"All mha2sha convert done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9146661",
   "metadata": {},
   "source": [
    "## Convert the model from ONNX representation to QNN DLC representation\n",
    "\n",
    "The Qualcomm AI Engine Direct SDK `qairt-converter` tool converts a model from ONNX representation to its equivalent QNN DLC representation. The encoding files generated from the AIMET workflow are provided as an input to this step via the `–quantization_overrides model.encodings` option.\n",
    "\n",
    "This step generates a `.dlc` file that represents the model as a series of QNN API calls.\n",
    "\n",
    "Expected execution time: ~< 20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25983b78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/artifacts/ar1-cl4096/1_of_1/sha_output/ar1-cl4096_1_of_1.onnx\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/artifacts/ar1-cl4096/1_of_1/sha_output/ar1-cl4096_1_of_1.onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m event_marker(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconvert-onnx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arn, split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(arn_list, split_idxs):\n\u001b[0;32m---> 36\u001b[0m         \u001b[43mthread_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43marn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll qairt-converter done.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m, in \u001b[0;36mthread_convert\u001b[0;34m(arn, split)\u001b[0m\n\u001b[1;32m     18\u001b[0m quantization_overrides\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_work_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/sha_output/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.encodings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m args \u001b[38;5;241m=\u001b[39m [QNN_SDK_ROOT \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/bin/x86_64-linux-clang/qairt-converter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--input_network\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_onnx,\n\u001b[1;32m     22\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--quantization_overrides\u001b[39m\u001b[38;5;124m\"\u001b[39m, quantization_overrides,\n\u001b[1;32m     23\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-o\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.dlc\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     24\u001b[0m                 ]\n\u001b[0;32m---> 25\u001b[0m options \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_onnx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musing_qairt_workflow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m options:\n\u001b[1;32m     27\u001b[0m     args\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mentry\n",
      "File \u001b[0;32m~/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/../../../common/G2G/split_onnx_utils/utils.py:733\u001b[0m, in \u001b[0;36mget_input_layout\u001b[0;34m(onnxfile, using_qairt_workflow)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m layout \u001b[38;5;129;01min\u001b[39;00m supported, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected layout:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, it should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    731\u001b[0m no_layout_option \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m#'attention_mask', 'position_ids']\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m onnxmodel \u001b[38;5;241m=\u001b[39m \u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43monnxfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_external_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_qairt_workflow:\n\u001b[1;32m    735\u001b[0m     input_info \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--source_model_input_layout\u001b[39m\u001b[38;5;124m\"\u001b[39m, i\u001b[38;5;241m.\u001b[39mname, layout)\n\u001b[1;32m    736\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m onnxmodel\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minput \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(j \u001b[38;5;129;01min\u001b[39;00m i\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m no_layout_option)]\n",
      "File \u001b[0;32m~/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/../../../common/G2G/split_onnx_utils/utils.py:719\u001b[0m, in \u001b[0;36m_load_model\u001b[0;34m(onnxfile, load_external_data, model_cache)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m onnxfile \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_cache:\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00monnxfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 719\u001b[0m     model_cache[onnxfile] \u001b[38;5;241m=\u001b[39m \u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43monnxfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_external_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_external_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_cache[onnxfile]\n",
      "File \u001b[0;32m~/.conda/envs/aimet/lib/python3.10/site-packages/onnx/__init__.py:210\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(f, format, load_external_data)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_model\u001b[39m(\n\u001b[1;32m    190\u001b[0m     f: IO[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike,\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mformat\u001b[39m: _SupportedFormat \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# noqa: A002\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     load_external_data: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    193\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelProto:\n\u001b[1;32m    194\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a serialized ModelProto into memory.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m        Loaded in-memory ModelProto.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     model \u001b[38;5;241m=\u001b[39m _get_serializer(\u001b[38;5;28mformat\u001b[39m, f)\u001b[38;5;241m.\u001b[39mdeserialize_proto(\u001b[43m_load_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m, ModelProto())\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m load_external_data:\n\u001b[1;32m    213\u001b[0m         model_filepath \u001b[38;5;241m=\u001b[39m _get_file_path(f)\n",
      "File \u001b[0;32m~/.conda/envs/aimet/lib/python3.10/site-packages/onnx/__init__.py:147\u001b[0m, in \u001b[0;36m_load_bytes\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     f \u001b[38;5;241m=\u001b[39m typing\u001b[38;5;241m.\u001b[39mcast(Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike], f)\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m readable:\n\u001b[1;32m    148\u001b[0m         content \u001b[38;5;241m=\u001b[39m readable\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/azureuser/zack/qnn-expr/llama32-compute/qwen3_mha_model/Step-2/host_linux/assets/artifacts/ar1-cl4096/1_of_1/sha_output/ar1-cl4096_1_of_1.onnx'"
     ]
    }
   ],
   "source": [
    "def thread_convert(arn,split):\n",
    "    model_artifact = f\"{workfolder}/assets/artifacts/ar{arn}-cl{CL}/\"\n",
    "    split_work_dir = os.path.join(model_artifact,f\"{split}_of_{num_splits}\")\n",
    "    name = f\"ar{arn}-cl{CL}_{split}_of_{num_splits}\"\n",
    "    os.makedirs(split_work_dir, exist_ok = True)\n",
    "    out_dir = os.path.join(split_work_dir, \"converted_model\")\n",
    "    os.makedirs(out_dir, exist_ok = True)\n",
    "    \n",
    "    # create symlink to export\n",
    "    for src in [f\"input_list_{name}.txt\",f\"test_inputs_{name}\"]:\n",
    "        symlink_input = os.path.join(split_work_dir, src)\n",
    "        symlink_path = Path(symlink_input)\n",
    "        if symlink_path.is_symlink():\n",
    "            os.unlink(symlink_input)\n",
    "        os.symlink(src = os.path.join(model_artifact, src), dst = symlink_input)       \n",
    "\n",
    "    input_onnx=f\"{split_work_dir}/sha_output/{name}.onnx\"\n",
    "    quantization_overrides= f\"{split_work_dir}/sha_output/{name}.encodings\"\n",
    "    \n",
    "    args = [QNN_SDK_ROOT + \"/bin/x86_64-linux-clang/qairt-converter\",\n",
    "                    \"--input_network\", input_onnx,\n",
    "                    \"--quantization_overrides\", quantization_overrides,\n",
    "                    \"-o\", f'{out_dir}/{name}.dlc'\n",
    "                    ]\n",
    "    options = utils.get_input_layout(input_onnx, using_qairt_workflow = True)\n",
    "    for entry in options:\n",
    "        args+=entry\n",
    "    \n",
    "    proc = subprocess.Popen(args, stdout = subprocess.PIPE, stderr = subprocess.PIPE, env = qnn_env)\n",
    "    output, error = proc.communicate()\n",
    "    print(output.decode(), error.decode())\n",
    "    print(f\"qairt-converter {name} done!\")\n",
    "\n",
    "with event_marker(f'convert-onnx'):\n",
    "    for arn, split in zip(arn_list, split_idxs):\n",
    "        thread_convert(arn, split)\n",
    "\n",
    "print(f\"All qairt-converter done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e40ecb",
   "metadata": {},
   "source": [
    "##  Quantized QNN DLC model\n",
    "\n",
    "The  Qualcomm AI Engine Direct SDK `qairt-quantizer` compiles the model `.dlc` and input`.raw` files into a `model.quantized.dlc` file.\n",
    "\n",
    "The inputs to this stage are the input raw files &  `model.dlc` generated in the previous step.\n",
    "\n",
    "Expected execution time: ~< 10 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a39542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def thread_genlib(arn,split):\n",
    "    model_artifact = f\"{workfolder}/assets/artifacts/ar{arn}-cl{CL}/\"\n",
    "    split_work_dir = os.path.join(model_artifact,f\"{split}_of_{num_splits}\")\n",
    "    name = f\"ar{arn}-cl{CL}_{split}_of_{num_splits}\"\n",
    "    os.chdir(split_work_dir)\n",
    "    out_dir = os.path.join(split_work_dir,\"compiled_model\")\n",
    "    os.makedirs( os.path.join(split_work_dir,\"compiled_model\"), exist_ok = True)\n",
    "\n",
    "    float_dlc_file = os.path.join(split_work_dir, \"converted_model\", f'{name}.dlc')\n",
    "    quantized_dlc_file = os.path.join(out_dir, f'{name}_quantized.dlc')  \n",
    "    ip_list_file = os.path.join(model_artifact, f'input_list_{name}.txt')\n",
    "    \n",
    "    proc = subprocess.Popen([QNN_SDK_ROOT + \"/bin/x86_64-linux-clang/qairt-quantizer\",\n",
    "                            \"--input_dlc\", float_dlc_file,\n",
    "                            \"--input_list\", ip_list_file,\n",
    "                            \"--output_dlc\", quantized_dlc_file,\n",
    "                            \"--act_bitwidth\", \"16\",\n",
    "                            \"--bias_bitwidth\", \"32\"\n",
    "                            ],stdout = subprocess.PIPE, stderr = subprocess.PIPE, env = qnn_env)\n",
    "    output, error = proc.communicate()\n",
    "    print(output.decode(), error.decode())\n",
    "    print(f\"qairt-quantizer {name} done!\")\n",
    "    os.chdir(workfolder)\n",
    "\n",
    "with event_marker(f'qairt-quantizer'):\n",
    "    for arn, split in zip(arn_list, split_idxs):\n",
    "        thread_genlib(arn, split)\n",
    "\n",
    "print(f\"All qairt-quantizer done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a07bd0",
   "metadata": {},
   "source": [
    "## QNN HTP weight sharing context binary\n",
    "\n",
    "The  Qualcomm AI Engine Direct SDK `qnn-context-binary-generator` tool creates a QNN context binary applicable to the QNN HTP backend. This binary can be deployed to run on a Snapdragon 8 Gen2 / Gen4 device that runs Android. This step requires the ar128 and ar1 quantized DLCs from the previous step and the `libQnnHtp.so` library, available in the Qualcomm AI Engine Direct SDK.\n",
    "\n",
    "Provide additional options that pertain to the QNN HTP backend by passing the `libQnnHtpBackendExtensions.so` library that implements extensions for the QNN HTP backend. The library is available in the Qualcomm AI Engine Direct SDK.\n",
    "\n",
    "### Define Htp Perf Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07439f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "\n",
    "def make_config_file(index, folder, src_graphs, soc_id=43, dsp_arch=\"v73\"): # For GEN4, set soc_id=69 and dsp_arch=\"v79\" here\n",
    "    htp_config_json = os.path.join(folder, f\"HtpConfigFile_API_{index}.json\")\n",
    "    perf_config_json = os.path.join(folder, f\"PerfSetting_API_{index}.conf\")\n",
    "\n",
    "    soc_id = int(soc_id)\n",
    "    with open(htp_config_json, 'w') as f:\n",
    "        config = {\n",
    "            \"backend_extensions\": {\n",
    "                \"shared_library_path\": \"libQnnHtpNetRunExtensions.so\", \n",
    "                \"config_file_path\": f\"{perf_config_json}\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "    with open(perf_config_json,'w') as f:\n",
    "        config = {\n",
    "            \"graphs\": [{\n",
    "                \"O\": 3.0,\n",
    "                \"vtcm_mb\": 8,\n",
    "                \"graph_names\": src_graphs,\n",
    "                \"fp16_relaxed_precision\": 0\n",
    "            }],\n",
    "            \"devices\": [\n",
    "                {\n",
    "                    \"soc_id\": soc_id,\n",
    "                    \"dsp_arch\": dsp_arch,\n",
    "                    \"cores\": [\n",
    "                        {\n",
    "                            \"perf_profile\": \"burst\",\n",
    "                            \"rpc_control_latency\": 100\n",
    "                        }\n",
    "                    ],\n",
    "                    \"pd_session\": \"unsigned\"\n",
    "                }\n",
    "            ], \n",
    "            \"context\": {\n",
    "                    \"weight_sharing_enabled\": len(src_graphs) > 1\n",
    "            }, \n",
    "            \"memory\": {\n",
    "                    \"mem_type\": \"shared_buffer\"\n",
    "            }\n",
    "        }\n",
    "        json.dump(config, f, indent = 4)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ed265",
   "metadata": {},
   "source": [
    "### Compile context binary\n",
    "Expected execution time: ~10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f30e5b",
   "metadata": {
    "lines_to_end_of_cell_marker": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "soc_id = nsp_target.soc_id\n",
    "dsp_arch = nsp_target.dsp_arch\n",
    "\n",
    "def thread_gen_ws_cb(i):\n",
    "    ar128_src = f\"{workfolder}/assets/artifacts/ar128-cl{CL}/\"\n",
    "    ar1_src = f\"{workfolder}/assets/artifacts/ar1-cl{CL}/\"\n",
    "    output_dir = f\"{workfolder}/assets/artifacts/ar128-ar1-cl{CL}_conf_files/\"\n",
    "    ctx_output_dir = f\"{workfolder}/assets/artifacts/ar128-ar1-cl{CL}/\"  \n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    os.makedirs(ctx_output_dir, exist_ok = True)\n",
    "\n",
    "    src1_split_folder = os.path.join(ar128_src, f\"{i}_of_{num_splits}\", \"compiled_model\")\n",
    "    src2_split_folder = os.path.join(ar1_src, f\"{i}_of_{num_splits}\", \"compiled_model\")\n",
    "\n",
    "    src1_graph_name = f\"ar128-cl{CL}_{i}_of_{num_splits}\"\n",
    "    src1_q_dlc = os.path.join(src1_split_folder, f\"{src1_graph_name}_quantized.dlc\")\n",
    "    src2_graph_name = f\"ar1-cl{CL}_{i}_of_{num_splits}\"\n",
    "    src2_q_dlc = os.path.join(src2_split_folder, f\"{src2_graph_name}_quantized.dlc\")\n",
    "\n",
    "    graph_list = [src1_graph_name, src2_graph_name]\n",
    "    make_config_file(i, output_dir, graph_list, soc_id, dsp_arch)\n",
    "\n",
    "    cmd = [\"qnn-context-binary-generator\",\n",
    "            \"--log_level=verbose\",\n",
    "            \"--backend\",\"libQnnHtp.so\",\n",
    "            \"--model\", \"libQnnModelDlc.so\",\n",
    "            \"--input_output_tensor_mem_type\", \"memhandle\",\n",
    "            \"--output_dir\", ctx_output_dir,\n",
    "            \"--config_file\",f\"{output_dir}/HtpConfigFile_API_{i}.json\",\n",
    "            \"--binary_file\", f\"weight_sharing_model_{i}_of_{num_splits}.serialized\",\n",
    "            \"--dlc_path\", f\"{src1_q_dlc},{src2_q_dlc}\"]\n",
    "    proc = subprocess.Popen(cmd, stdout = subprocess.PIPE, stderr = subprocess.PIPE, env = qnn_env)\n",
    "    output, error = proc.communicate()\n",
    "    print(output.decode(), error.decode())\n",
    "    print(f'#{i} weight sharing model generated')\n",
    "\n",
    "with event_marker(f'gen-binary'):\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers = len(splits) if go_parallel else 1) as executor:\n",
    "        results = executor.map(thread_gen_ws_cb, splits)\n",
    "print(f\"All weight shared qnn-context-binary generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455bd8d",
   "metadata": {},
   "source": [
    "### Save profiling stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.profiler import EventProfiler\n",
    "EventProfiler().report()\n",
    "EventProfiler().json_dump(os.path.join(workfolder, 'assets/profiling_stats.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d964165",
   "metadata": {},
   "source": [
    "Upon completion of these steps to prepare TinyLlama 1.1B model for inference, QNN context binaries  are available in `./assets/artifacts`. \n",
    "The next step is to execute the prepared models (now represented as serialized context binaries) on a Snapdragon 8 Gen2 / Gen4 Android device using executable utilities available in the Qualcomm AI Engine Direct SDK.\n",
    "\n",
    "\n",
    "Copyright (c) 2024 Qualcomm Technologies, Inc. and/or its subsidiaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023b642b-ad29-480e-b542-0fd17207374f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
