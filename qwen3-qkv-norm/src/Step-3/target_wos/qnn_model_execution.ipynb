{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QNN Model Execution on Windows Snapdragon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide describes how to execute the Llama model on Snapdragon X Elite using the Qualcomm AI Engine Direct SDK.\n",
    "\n",
    "This document uses the terms Qualcomm Neural Network (QNN) and Qualcomm AI Engine Direct SDK interchangeably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "1. Qualcomm AI Engine Direct SDK (with Windows arm64 support)\n",
    "2. QNN context binary generated by qnn_model_prepare.ipynb\n",
    "4. Snapdragon X Elite Windows device\n",
    "6. A Jupyter environment as mentioned in qnn_model_prepare.ipynb and qnn_model_execution.ipynb\n",
    "7. Model preparation completed as specified in qnn_model_prepare.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Platform requirements\n",
    "- Platform: ARM64\n",
    "- Python 3.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "1. Configure a Qualcomm Windows device (preferably with a Snapdragon X Elite)\n",
    "3. Copy the prepared model files (generated from the qnn_model_prepare_on_linux.ipynb notebook) and libraries (from QNN SDK) on the Windows device\n",
    "4. Run the models in a Llama pipeline. Given a user prompt, execute the models as a Llama pipeline on QNN HTP on the Android platform to produce a human-like response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Create and activate the Python 3.10 virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure a Qualcomm Snapdragon Windows device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genai t2t run device target set up \n",
    "target_genai_t2t_run_path = \"target_genai_t2t_run\"\n",
    "os.makedirs(target_genai_t2t_run_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Update QNN SDK Path\n",
    "QNN_SDK = os.path.join(\"<QNN SDK Path>\")\n",
    "# Example given below:\n",
    "# QNN_SDK = os.path.join(\"C:\\\\Qualcomm\\\\AIStack\\\\QAIRT\\\\2.28.0.241029\")\n",
    "\n",
    "# assert os.path.exists(QNN_SDK), \"Please enter the correct location of QNN SDK Root\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up NSP target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../../')\n",
    "sys.path.append('../../../common/')\n",
    "sys.path.append('../../../common/utilities')\n",
    "\n",
    "from nsptargets import NspTargets\n",
    "\n",
    "# Set up nsp target specification\n",
    "# see common/utilities/nsptargets.py for other supported targets\n",
    "nsp_target = NspTargets.Windows.GEN2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow for genie-t2t-run\n",
    "\n",
    "1. Push genie t2t run dependency files, the files include the following:\n",
    "    - genie-t2t-run.exe\n",
    "    - QnnHtp.dll, QnnHtp.lib, QnnHtpNetRunExtensions.dll, QnnSystem.dll\n",
    "    - QnnHtpV73Stub.dll, libQnnHtpV73Skel.so, libQnnHtpV73Skel.so, libqnnhtpv73Skel.cat (v73 refers to the Windows Gen2 NSP target)\n",
    "    - Genie.dll\n",
    "    - tokenizer_llama32.json\n",
    "2. Generate htp_model_config_data and htp_backend_config_data configs\n",
    "3. Run script on device that executes genie-t2t-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_file = \"tokenizer_llama32.json\"\n",
    "backend_config_file = \"htp_backend_ext_config.json\"\n",
    "model_config_file = \"llama32-3b-htp-model-config.json\"\n",
    "\n",
    "# push needed files\n",
    "def copy_t2t_dependency_files():\n",
    "    # Push qnn sdk libraries\n",
    "    libs = [\"QnnHtp.dll\", \"QnnHtp.lib\", \"QnnHtpNetRunExtensions.dll\", \"QnnSystem.dll\", nsp_target.qnn_htp_lib_name+\"Stub.dll\"]\n",
    "    libs_dir = os.path.join(QNN_SDK, \"lib\" , \"aarch64-windows-msvc\")\n",
    "    for lib in libs:\n",
    "        src = os.path.join(libs_dir, lib)\n",
    "        dest = os.path.join(target_genai_t2t_run_path, lib)\n",
    "        shutil.copyfile(src, dest)\n",
    "        \n",
    "    skel_dir = os.path.join(QNN_SDK, \"lib\", f\"hexagon-{nsp_target.dsp_arch}\", \"unsigned\")\n",
    "    skel_file = f\"lib{nsp_target.qnn_htp_lib_name}Skel.so\"\n",
    "    src = os.path.join(skel_dir, skel_file)\n",
    "    dest = os.path.join(target_genai_t2t_run_path, skel_file)\n",
    "    shutil.copyfile(src, dest)\n",
    "\n",
    "    so_file = f\"lib{nsp_target.qnn_htp_lib_name}.so\"\n",
    "    src = os.path.join(skel_dir, so_file)\n",
    "    dest = os.path.join(target_genai_t2t_run_path, so_file)\n",
    "    shutil.copyfile(src, dest)\n",
    "\n",
    "    skel_cat_file = f\"lib{nsp_target.qnn_htp_lib_name.lower()}.cat\"\n",
    "    src = os.path.join(skel_dir, skel_cat_file)\n",
    "    dest = os.path.join(target_genai_t2t_run_path, skel_cat_file)\n",
    "    shutil.copyfile(src, dest)\n",
    "\n",
    "    # Push QNN Genie libraries \n",
    "    libs = [\"Genie.dll\"]\n",
    "    for lib in libs:\n",
    "        src = os.path.join(libs_dir, lib)\n",
    "        dest = os.path.join(target_genai_t2t_run_path, lib)\n",
    "        shutil.copyfile(src, dest)\n",
    "\n",
    "    # Copy tokenizer.json\n",
    "    src = os.path.join(\"./\", tokenizer_file)\n",
    "    dest = os.path.join(target_genai_t2t_run_path, tokenizer_file)\n",
    "    shutil.copyfile(src, dest)\n",
    "    \n",
    "    genie_executable = \"genie-t2t-run.exe\"\n",
    "    t2t_net_run = os.path.join(QNN_SDK, \"bin\" , \"aarch64-windows-msvc\", genie_executable)\n",
    "    dest = os.path.join(target_genai_t2t_run_path, genie_executable)\n",
    "    shutil.copyfile(t2t_net_run, dest)\n",
    "\n",
    "# Inference script \n",
    "def create_t2t_runtime_script(context_binary_paths, config_folder):    \n",
    "    with open(backend_config_file, 'r') as r1:\n",
    "        backend_data = json.load(r1)\n",
    "        # You can make any changes for backend config file here\n",
    "\n",
    "        with open(os.path.join(config_folder, backend_config_file), 'w') as f1:\n",
    "            f1.write(json.dumps(backend_data, indent=4))\n",
    "\n",
    "    with open(model_config_file, 'r') as r2:\n",
    "        model_data = json.load(r2)\n",
    "        # You can make any changes for backend config file here\n",
    "        model_data[\"dialog\"][\"tokenizer\"][\"path\"] = tokenizer_file\n",
    "        model_data[\"dialog\"][\"engine\"][\"model\"][\"binary\"][\"ctx-bins\"] = context_binary_paths\n",
    "        # You can uncomment the command given below and\n",
    "        # change the context size based on the value of context length while generating the notebooks.\n",
    "        model_data[\"dialog\"][\"context\"][\"size\"] = 4096\n",
    "\n",
    "        with open(os.path.join(config_folder, model_config_file), 'w') as f2:\n",
    "            f2.write(json.dumps(model_data, indent=4))\n",
    "\n",
    "def run_t2t_on_target(prompt, target_genai_t2t_run_path):\n",
    "    command = [ \"powershell.exe\", \".\\genie-t2t-run.exe\",\n",
    "                # \"-b\", \"QnnHtp.dll\", # uncomment this line if the QNN version is <= 2.25\n",
    "                \"-c\", model_config_file,\n",
    "                \"-p\", f\"\\\"{prompt}\\\"\"\n",
    "                ]\n",
    "    result = subprocess.Popen(command, stdout = subprocess.PIPE, stderr = subprocess.PIPE, cwd=target_genai_t2t_run_path)\n",
    "    output, error = result.communicate()\n",
    "\n",
    "    print(\"Output:\", output.decode())\n",
    "    print(\"Error:\", error.decode())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run llama with genie-t2t-run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : Copy the context binaries to \"target_genai_t2t_run_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Replace the below names with the absolute paths for all the 3 serialized binaries\n",
    "WEIGHT_SHARE_CONTEXT_BINARY_PATH = [\n",
    "    \"<ctx_bin_1_of_3>.bin\",\n",
    "    \"<ctx_bin_2_of_3>.bin\",\n",
    "    \"<ctx_bin_3_of_3>.bin\"\n",
    "]\n",
    "\n",
    "prompt_template = (\n",
    "    \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "    \"{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "    \"{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    ")\n",
    "\n",
    "# 3. Update the system and user prompts to be used for the execution.\n",
    "sys_prompt = \"You are an helpful assistant.\"\n",
    "user_prompt = \"Plan a three day trip to San Diego\"\n",
    "prompt = prompt_template.format_map({'instruction': user_prompt, 'system_prompt': sys_prompt})\n",
    "\n",
    "isFailed = False\n",
    "for i in range(len(WEIGHT_SHARE_CONTEXT_BINARY_PATH)):\n",
    "    context_binary_filename = os.path.basename(WEIGHT_SHARE_CONTEXT_BINARY_PATH[i])\n",
    "    if not os.path.exists(WEIGHT_SHARE_CONTEXT_BINARY_PATH[i]):\n",
    "        print(f\"Context binary: ({context_binary_filename}) does not exists in this path {WEIGHT_SHARE_CONTEXT_BINARY_PATH[i]}.\")\n",
    "        isFailed = True\n",
    "        break\n",
    "        \n",
    "# Create configs,scripts and copy dependencies\n",
    "if isFailed is False :\n",
    "    create_t2t_runtime_script(WEIGHT_SHARE_CONTEXT_BINARY_PATH, target_genai_t2t_run_path)\n",
    "    copy_t2t_dependency_files()\n",
    "\n",
    "# Run the model \n",
    "if isFailed is False :\n",
    "    run_t2t_on_target(prompt, target_genai_t2t_run_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Qualcomm Technologies, Inc. and/or its subsidiaries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
